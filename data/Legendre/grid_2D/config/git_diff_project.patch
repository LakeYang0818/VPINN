diff --git a/README.md b/README.md
index 339a343..c6f8ba7 100644
--- a/README.md
+++ b/README.md
@@ -15,6 +15,24 @@ is implemented using [Pytorch](https://pytorch.org/tutorials/) and is unit teste
 > overview of the Utopia syntax. You can find a complete guide on running models with Utopia/utopya
 > [here](https://docs.utopia-project.org/html/getting_started/tutorial.html#tutorial).

+> **_Hint_**: If you encounter any difficulties, please [file an issue](https://github.com/ThGaskin/VPINN/issues/new).
+>
+### Contents of this README
+* [How to install](#how-to-install)
+* [How to run the model](#how-to-run-the-model)
+* [Plotting](#plotting)
+* [Configuration sets](#configuration-sets)
+* [Modifying the configuration](#modifying-the-configuration)
+  * [Grid settings](#grid-settings)
+  * [Function settings](#function-settings)
+* [How to adjust the neural net configuration](#how-to-adjust-the-neural-net-configuration)
+  * [Training settings](#training-settings)
+* [Generating and loading grid and test function data](#generating-and-loading-grid-and-test-function-data)
+* [Parameter sweeps](#parameter-sweeps)
+* [Tests (WIP)](#-tests-wip)
+
+---
+
 ## How to install
 #### 1. Clone this repository
 Clone this repository using a link obtained from 'Code' button:
@@ -60,9 +78,9 @@ utopya models register from-manifest model/VPINN_info.yml
 ```
 Done! ðŸŽ‰

-#### 4. Download data using git LFS (optional)
-The repository contains some pre-processed grid and test function data, which speeds up running models considerably,
-since you do not need to generate the training data. These files are stored using [git LFS](https://git-lfs.github.com)
+#### 4. Download pre-generated grid and test function data using git LFS (optional)
+The repository contains some prepared grid and test function data that can be used to train the model.
+Using it speeds up the runtime considerably. These files are stored using [git LFS](https://git-lfs.github.com)
 (large file storage). To download them, first install git lfs via
 ```console
 git lfs install
@@ -73,7 +91,7 @@ git lfs pull
 ```
 This will pull all the datasets.

-## How to run a model
+## How to run the model
 Now you have set up the model, run it by invoking the basic run command
 ```console
 utopya run VPINN
@@ -84,16 +102,16 @@ It will generate a synthetic dataset of test function and grid data, train the n
 `~/utopya_output` by default.

 The default configuration settings are provided in the `VPINN_cfg.yml` file in the
-`model` folder. You can modify the settings here, but we recommend changing the configuration
+`model` folder. You can modify the settings there, but we recommend changing the configuration
 settings by instead creating a `run.yml` file somewhere and using that to run the model. You can do so by
 calling
 ```console
 utopya run VPINN path/to/run_cfg.yml
 ```
 In this file, you only need to specify those entries from the `VPINN_cfg.yml` file you wish to change,
-and not reproduce the entire configuration set. The advantage of this approach is that you can
+not copy the entire configuration. The advantage of this approach is that you can
 create multiple configs for different scenarios, and leave the working base configuration untouched.
-An example could look like this:
+An example ``run.yml`` could look like this:

 ```yaml
 parameter_space:
@@ -136,32 +154,32 @@ utopya eval VPINN --eval-cfg path/to/eval/cfg
 Even more convenient are so-called *configuration* sets:

 ## Configuration sets
-Configuration sets are bundled run and evaluation configuration configurations. Take a look at the `models/cfgs` folder:
+Configuration sets are bundled run and evaluation configurations. Take a look at the `models/cfgs` folder:
 it contains a number of examples. To run and evaluate the model from one of these configuration sets, just call
 ```console
-utopya run VPINN --cfg-set cfg_set_name
+utopya run VPINN --cfg-set <cfg_set_name>
 ```
-replacing `cfg_set_name` with the name of the configuration set. To add a new set, simply create a new
-folder in the `cfgs` folder (or anywhere else, but then taking care to pass an absolute path
+replacing `<cfg_set_name>` with the name of the configuration set. To add a new set, simply create a new
+folder in the `cfgs` folder (or anywhere else, but then take care to pass an absolute path
 rather than just a name). Running the configuration set will produce plots. If you wish to re-evaluate a run (perhaps plotting different figures),
 you do not need to re-run the model, since the data has already been generated. Simply call

 ```console
-utopya eval VPINN --cfg-set <name_of_cfg_set>
+utopya eval VPINN --cfg-set <cfg_set_name>
 ```

-This will re-evaluate the *last model you ran*. You can re-evaluate any dataset, of course, by
+This will re-evaluate the *last model you ran*. You can re-evaluate any run, of course, by
 providing the path to that dataset, as before:

 ```console
-utopya eval VPINN path/to/output/folder --cfg-set <name_of_cfg_set>
+utopya eval VPINN path/to/output/folder --cfg-set <cfg_set_name>
 ```
 ## Modifying the configuration
 To control the simulation, modify the entries in your run configuration. There are
-a number of settings you can adjust. Remember, the run configuration only requires those entries to
+a number of settings you can adjust. Remember, the run configuration only requires those entries you
 wish to change with respect to the default values.
 ### Grid settings
-The domain is controlled from `space` entry:
+The grid is controlled from `space` entry:

 ```yaml
 parameter_space:
@@ -183,6 +201,7 @@ you can pass a `predictions_grid` entry:
 VPINN:
   predictions_grid:
     x:
+      extent: [2, 3]
       size: 100
 ```
 The model will recursively update any entries from the `space` configuration and use these
@@ -212,7 +231,7 @@ The test function weighting is controlled from `test_functions/weight_function`
 weighting can be either `uniform` (all test functions have weight 1), or `exponential`, meaning
 the `kl`-th function has weight `2**{-(k+l)}`.

-## How to adjust the neural net configurations
+## How to adjust the neural net configuration
 You can vary the size of the neural net and the activation functions
 right from the config. The size of the input layer is inferred from
 the data passed to it, and the size of the output layer is
@@ -310,10 +329,25 @@ Note that the indexing range of the boundary begins (for two-dimensional grids)
 grid boundary anti-clockwise. For three dimensional grids, the index begins on the lower left corner of the
 front panel, wraps anti-clockwise around the sides, then selects the upper, and finally the lower panel.

-## Loading data
-Once you have generated grid and test function data, you can reuse it to train the neural net repeatedly. This will
+## Generating and loading grid and test function data
+You can generate and reuse grid and test function data to train the neural net repeatedly. This will
 speed up computations enormously, and also allow sweep runs with different neural net configurations.
-To load a dataset, pass the path to the directory containing the `.h5` file to load to the load configuration entry of the config:
+
+To generate grid and test function data, set the ``generation_run`` key to ``True``:
+
+```yaml
+parameter_space:
+  generation_run: True
+```
+See the ``cfgs/Grid_generation_2D`` configuration set for an example. This will generate a dataset and save it to the
+``data/uni0`` folder in the output folder, which you can then use to train a model. The dataset will contain the grid,
+its boundary, and the test functions and their derivatives evaluated on the grid and the grid boundary.
+You can set the number of test functions higher than you actually require, as you can later always select a subset of
+test functions to use for training (see below). The dataset does *not* contain any information on the external forcing or
+boundary conditions, wherefore it can be used for different equations and function examples.
+
+To load a dataset, pass the path to the directory containing the `.h5` file to load to the
+load configuration entry of the config:

 ```yaml
 VPINN:
@@ -330,9 +364,23 @@ VPINN:
     data_dir: path/to/folder
     copy_data: True
 ```
+You may also wish to generate a large dataset of many test functions in advance, but then
+train the model on a smaller subset. This can be done by adding the following entry to the
+``load_data`` key:
+
+```yaml
+load_data:
+  test_function_subset:
+    n_x: !slice [~, 4]   # Chooses the first four test functions in x direction.
+```
+Use the ``!slice`` tag to make a selection using the standard python ``(start, stop, step)`` syntax, with ``~`` indicating ``None`` in yaml.
+For example, ``!slice [~, ~, 2]`` would select every second test function.
+
+> **_Warning_**:
+> Make sure the names of the test function indices (``n_x`` in this example) match the keys in the dataset.

-To turn off data loading, set the ``data_dir`` entry to ``~`` (``None`` in yaml), or delete the
-``load_data`` entry entirely.
+To turn off data loading, set the ``data_dir`` key to ``~`` (``None`` in yaml), or delete the
+``load_data`` entry entirely. See the ``Burgers1+1D`` or ``Poisson2D`` configuration sets for examples.

 ## Parameter sweeps
 > **_Note_**: Take a look at the [full tutorial entry](https://docs.utopia-project.org/html/getting_started/tutorial.html#parameter-sweeps)
diff --git a/data/Burger1+1D/_report.txt b/data/Burger1+1D/_report.txt
deleted file mode 100644
index 14111eb..0000000
--- a/data/Burger1+1D/_report.txt
+++ /dev/null
@@ -1,4 +0,0 @@
-Universe Runtimes
------------------
-
-  uni0          2m 58s
\ No newline at end of file
diff --git a/data/Poisson_SinSin2D/config/base_cfg.yml b/data/Poisson_SinSin2D/config/base_cfg.yml
deleted file mode 100644
index 43faede..0000000
--- a/data/Poisson_SinSin2D/config/base_cfg.yml
+++ /dev/null
@@ -1,518 +0,0 @@
-# This file provides the basic configuration for the utopya Multiverse
-#
-# It is read in by the Multiverse during initialization and is subsequently
-# updated by other configuration files to generate the meta configuration of
-# the Multiverse, which determines all details of how a run is performed.
-#
-# The top-level keys here are used to configure different parts of Multiverse:
-#   - properties of the Multiverse itself: `paths`, `perform_sweep`
-#   - properties of attributes: `worker_manager`, `run_kwargs`, ...
-#   - and the parameter space that is passed on to the model instance
-#
-# NOTE that this configuration file documents some features in the comments.
-#      This cannot be exhaustive. Check the docstrings of the functions for
-#      further information.
----
-# Multiverse configuration ....................................................
-# Output paths
-paths:
-  # base output directory
-  out_dir: ~/utopya_output
-
-  # model note is added to the output directory path
-  model_note: ~
-
-  # From the two above, the run directory will be created at:
-  #     <out_dir>/<model_name>/<timestamp>-<model_note>/
-  # Subfolders will be:  config, eval, data
-
-# Control of the backup of files that belong to a simulation
-backups:
-  # Whether to save all involved config files granularly, i.e. one by one.
-  # If false, only the resulting meta_cfg is saved to the config subdirectory.
-  backup_cfg_files: true
-
-  # Whether to save the executable
-  backup_executable: false
-
-  # Whether to store git information of the project (and framework)
-  include_git_info: true
-
-# Control of the model executable
-executable_control:
-  # Whether to copy the executable to a temporary directory at the
-  # initialization of the Multiverse and execute it from there. This way,
-  # accidental changes to the executable _during_ a simulation are prevented.
-  run_from_tmpdir: true
-
-# Whether to perfom a parameter sweep
-# Is evaluated by the Multiverse.run method
-perform_sweep: false
-# NOTE This will be ignored if run_single or run_sweep are called directly.
-#      Also, the `parameter_space` key (see below) will need to span at least
-#      a volume of 1 in order to be sweep-able.
-
-# Whether to perform parameter validation
-# For large sweeps, validation can take quite some time. For such scenarios, it
-# might make sense to disable parameter validation by setting this to false.
-perform_validation: true
-
-# Parameters that are to be validated
-# This is a mapping of key sequence -> Parameter object
-parameters_to_validate: {}
-
-
-
-# Reporter ....................................................................
-# The Multiverse owns a Reporter object to report on the progress of the
-# WorkerManager. Part of its configuration happens using its init kwargs, which
-# are defined in the following.
-# The rest of the configuration happens on the WorkerManager-side (see there).
-reporter:
-  # Define report formats, which are accessible, e.g. from the WorkerManager
-  report_formats:
-    progress_bar:                     # Name of the report format specification
-      parser: progress_bar            # The parser to use
-      write_to: stdout_noreturn       # The writer to use
-      min_report_intv: 0.5            # Required time (in s) between writes
-
-      # -- All further kwargs on this level are passed to the parser
-      # Terminal width for the progress bar
-      # Can also be `adaptive` (poll each time), or `fixed` (poll once)
-      num_cols: adaptive
-
-      # The format string to use for progress information
-      # Available keys:
-      #   - `total_progress` (in %)
-      #   - `active_progress` (mean progress of _active_ simulations, in %)
-      #   - `cnt` (dict of counters: `total`, `finished`, `active`)
-      info_fstr: "{total_progress:>5.1f}% "
-      # Example of how to access counters in format string:
-      # info_fstr: "finished {cnt[finished]}/{cnt[total]} "
-
-      # Whether to show time information alongside the progress bar
-      show_times: true
-
-      # How to display time information.
-      # Available keys: `elapsed`, `est_left`, `est_end`, `start`, `now`
-      # (see `times` parser for more information)
-      times_fstr: "| {elapsed:>7s} elapsed | ~{est_left:>7s} left "
-      times_fstr_final: "| finished in {elapsed:} "
-      times_kwargs:
-        # How to compute the estimated time left to finish the work session
-        # Available modes:
-        #   - `from_start`:  extrapolates from progress made since start
-        #   - `from_buffer`: uses a buffer to store recent progress
-        #                    information and use the oldest value for
-        #                    making the estimate; see `progress_buffer_size`
-        mode: from_buffer
-
-        # Number of records kept for computing ETA in `from_buffer` mode.
-        # This is in units of parser invocations, so goes back *at least* a
-        # a time interval of `min_report_intv * progress_buffer_size`.
-        # If the reporter is called less frequently (e.g. because of a larger
-        # model-side `monitor_emit_interval`), this interval will be longer.
-        progress_buffer_size: 90
-
-    # Creates a report file containing runtime statistics
-    report_file:
-      parser: report
-      write_to:
-        file:
-          path: _report.txt
-      min_report_intv: 10             # don't update this one too often
-      min_num: 4                      # min. number of universes for statistics
-      show_individual_runtimes: true  # for large number of universes, disable
-      task_label_singular: universe
-      task_label_plural: universes
-
-    # Creates a parameter sweep information file
-    sweep_info:
-      parser: pspace_info
-      write_to:
-        file:
-          path: _sweep_info.txt
-          skip_if_empty: true
-        log:
-          lvl: 18
-          skip_if_empty: true
-      fstr: "Sweeping over the following parameter space:\n\n{sweep_info:}"
-
-  # Can define a default format to use
-  # default_format: ~
-
-
-# Worker Manager ..............................................................
-# Initialization arguments for the WorkerManager
-worker_manager:
-  # Specify how many processes work in parallel
-  num_workers: auto
-  # can be: an int, 'auto' (== #CPUs). For values <= 0: #CPUs - num_workers
-
-  # Delay between polls [seconds]
-  poll_delay: 0.05
-  # NOTE: If this value is too low, the main thread becomes very busy.
-  #       If this value is too high, the log output from simulations is not
-  #       read from the line buffer frequently enough.
-
-  # Maximum number of lines to read from each task's stream per poll cycle.
-  # Choosing a value that is too large may affect poll performance in cases
-  # where the task generates many lines of output.
-  # Set to -1 to read *all* available lines from the stream upon each poll.
-  lines_per_poll: 20
-
-  # Periodic task callback (in units of poll events). Set None to deactivate.
-  periodic_task_callback: ~
-
-  # How to react upon a simulation exiting with non-zero exit code
-  nonzero_exit_handling: raise
-  # can be: ignore, warn, warn_all, raise
-  # warn_all will also warn if the simulation was terminated by the frontend
-  # raise will lead to a SystemExit with the error code of the simulation
-
-  # How to handle keyboard interrupts
-  interrupt_params:
-    # Which signal to send to the workers
-    send_signal: SIGINT  # can be any valid signal name
-    # NOTE that only SIGINT and SIGTERM lead to a graceful shutdown on C++ side
-
-    # How long to wait for workers to shut down before calling SIGKILL on them
-    grace_period: 5.
-    # WARNING Choosing a grace period that is shorter than the duration of one
-    #         iteration step of your model might lead to corrupted HDF5 data!
-
-    # Whether to exit after working; exit code will be 128 + abs(signum)
-    exit: false
-
-  # In which events to save streams *during* the work session
-  # May be: `monitor_updated`, `periodic_callback`
-  save_streams_on: [monitor_updated]
-
-  # Reporters to invoke at different points of the WorkerManager's operation.
-  # Keys refer to events, values are lists of report format names, which can be
-  # defined via the WorkerManagerReporter (see `reporter.report_formats` above)
-  rf_spec:
-    before_working: [sweep_info]
-    while_working: [progress_bar]
-    task_spawned: [progress_bar]
-    monitor_updated: [progress_bar]
-    task_finished: [progress_bar, report_file]
-    after_work: [progress_bar, report_file]
-    after_abort: [progress_bar, report_file]
-
-
-# Configuration for the WorkerManager.start_working method
-run_kwargs:
-  # Total timeout (in s) of a run; to ignore, set to ~
-  timeout: ~
-
-  # A list of StopCondition objects to check during the run _for each worker_.
-  # The entries of the following list are OR-connected, i.e. it suffices that
-  # one is fulfilled for the corresponding worker to be stopped
-  stop_conditions: ~
-  # See docs for how to set these up:
-  #   https://docs.utopia-project.org/html/usage/run/stop-conditions.html
-
-
-# The defaults for the worker_kwargs
-# These are passed to the setup function of each WorkerTask before spawning
-worker_kwargs:
-  # Whether to save the streams of each Universe to a log file
-  save_streams: true
-  # This file is saved only after the WorkerTask has finished in order to
-  # reduce I/O operations on files
-
-  # Whether to forward the streams to stdout
-  forward_streams: in_single_run
-  # can be: true, false, or 'in_single_run' (print only in single runs)
-
-  # Whether to forward the raw stream output or only those lines that were not
-  # parsable to yaml, i.e.: only the lines that came _not_ from the monitor
-  forward_raw: true
-
-  # The log level at which the streams should be forwarded to stdout
-  streams_log_lvl: ~  # if None, uses print instead of the logging module
-
-  # Arguments to subprocess.Popen
-  popen_kwargs:
-    # The encoding of the streams (STDOUT, STDERR) coming from the simulation.
-    # NOTE If your locale is set to some other encoding, or the simulation uses
-    #      a custom one, overwrite this value accordingly via the user config.
-    encoding: utf8
-
-
-# Cluster mode configuration ..................................................
-# Whether cluster mode is enabled
-cluster_mode: false
-
-# Parameters to configure the cluster mode
-cluster_params:
-  # Specify the workload manager to use.
-  # The names of environment variables are chosen accordingly.
-  manager: slurm   # available:  slurm
-
-  # The environment to look for parameters in. If not given, uses os.environ
-  env: ~
-
-  # Specify the name of environment variables for each supported manager
-  # The resolved values are available at the top level of the dict that is
-  # returned by Multiverse.resolved_cluster_params
-  env_var_names:
-    slurm:
-      # --- Required variables ---
-      # ID of the job
-      job_id: SLURM_JOB_ID
-
-      # Number of available nodes
-      num_nodes: SLURM_JOB_NUM_NODES
-
-      # List of node names
-      node_list: SLURM_JOB_NODELIST
-
-      # Name of the current node
-      node_name: SLURMD_NODENAME  # sic!
-
-      # This is used for the name of the run
-      timestamp: RUN_TIMESTAMP
-
-      # --- Optional values ---
-      # Name of the job
-      job_name: SLURM_JOB_NAME
-
-      # Account from which the job is run
-      job_account: SLURM_JOB_ACCOUNT
-
-      # Number of processes on current node
-      num_procs: SLURM_CPUS_ON_NODE
-
-      # Cluster name
-      cluster_name: SLURM_CLUSTER_NAME
-
-      # Custom output directory
-      custom_out_dir: UTOPIA_CLUSTER_MODE_OUT_DIR
-
-    # Could have more managers here, e.g.: docker
-
-  # Which parser to use to extract node names from node list
-  node_list_parser_params:
-    slurm: condensed  # e.g.: node[002,004-011,016]
-
-  # Which additional info to include into the name of the run directory, i.e.
-  # after the timestamp and before the model directory. All information that
-  # is extracted from the environment variables is available as keyword
-  # argument to format. Should be a sequence of format strings.
-  additional_run_dir_fstrs: [ "job{job_id:}" ]
-
-
-# Data Manager ................................................................
-# The DataManager takes care of loading the data into a tree-like structure
-# after the simulations are finished.
-# It is based on the DataManager class from the dantro package. See there for
-# full documentation.
-data_manager:
-  # Where to create the output directory for this DataManager, relative to
-  # the run directory of the Multiverse.
-  out_dir: eval/{timestamp:}
-  # The {timestamp:} placeholder is replaced by the current timestamp such that
-  # future DataManager instances that operate on the same data directory do
-  # not create collisions.
-  # Directories are created recursively, if they do not exist.
-
-  # Define the structure of the data tree beforehand; this allows to specify
-  # the types of groups before content is loaded into them.
-  # NOTE The strings given to the Cls argument are mapped to a type using a
-  #      class variable of the DataManager
-  create_groups:
-    - path: multiverse
-      Cls: MultiverseGroup
-
-  # Where the default tree cache file is located relative to the data
-  # directory. This is used when calling DataManager.dump and .restore without
-  # any arguments, as done e.g. in the Utopia CLI.
-  default_tree_cache_path: data/.tree_cache.d3
-
-  # Supply a default load configuration for the DataManager
-  # This can then be invoked using the dm.load_from_cfg() method.
-  load_cfg:
-    # Load the frontend configuration files from the config/ directory
-    # Each file refers to a level of the configuration that is supplied to
-    # the Multiverse: base <- user <- model <- run <- update
-    cfg:
-      loader: yaml                          # The loader function to use
-      glob_str: 'config/*.yml'              # Which files to load
-      ignore:                               # Which files to ignore
-        - config/parameter_space.yml
-        - config/parameter_space_info.yml
-        - config/full_parameter_space.yml
-        - config/full_parameter_space_info.yml
-        - config/git_info_project.yml
-        - config/git_info_framework.yml
-      required: true                        # Whether these files are required
-      path_regex: config/(\w+)_cfg.yml      # Extract info from the file path
-      target_path: cfg/{match:}             # ...and use in target path
-
-    # Load the parameter space object into the MultiverseGroup attributes
-    pspace:
-      loader: yaml_to_object                # Load into ObjectContainer
-      glob_str: config/parameter_space.yml
-      required: true
-      load_as_attr: true
-      unpack_data: true                     # ... and store as ParamSpace obj.
-      target_path: multiverse
-
-    # Load the configuration files that are generated for _each_ simulation
-    # These hold all information that is available to a single simulation and
-    # are in an explicit, human-readable form.
-    uni_cfg:
-      loader: yaml
-      glob_str: data/uni*/config.yml
-      required: true
-      path_regex: data/uni(\d+)/config.yml
-      target_path: multiverse/{match:}/cfg
-      parallel:
-        enabled: true
-        min_files: 1000
-        min_total_size: 1048576  # 1 MiB
-
-    # Example: Load the binary output data from each simulation.
-    # data:
-    #   loader: hdf5_proxy
-    #   glob_str: data/uni*/data.h5
-    #   required: true
-    #   path_regex: data/uni(\d+)/data.h5
-    #   target_path: multiverse/{match:}/data
-    #   enable_mapping: true   # see DataManager for content -> type mapping
-
-    #   # Options for loading data in parallel (speeds up CPU-limited loading)
-    #   parallel:
-    #     enabled: false
-
-    #     # Number of processes to use; negative is deduced from os.cpu_count()
-    #     processes: ~
-
-    #     # Threshold values for parallel loading; if any is below these
-    #     # numbers, loading will *not* be in parallel.
-    #     min_files: 5
-    #     min_total_size: 104857600  # 100 MiB
-
-    # The resulting data tree is then:
-    #  â””â”¬ cfg
-    #     â””â”¬ base
-    #      â”œ meta
-    #      â”œ model
-    #      â”œ run
-    #      â”” update
-    #   â”” multiverse
-    #     â””â”¬ 0
-    #        â””â”¬ cfg
-    #         â”” data
-    #           â””â”€ ...
-    #      â”œ 1
-    #      ...
-
-
-# Plot Manager ................................................................
-# The PlotManager, also from the dantro package, supplies plotting capabilities
-# using the data in the DataManager.
-plot_manager:
-  # Save the plots to the same directory as that of the data manager
-  out_dir: ""
-
-  # Whether to raise exceptions for plotting errors. false: only log them
-  raise_exc: false
-
-  # How to handle already existing plot configuration files
-  cfg_exists_action: raise
-  # NOTE If in cluster mode, this value is set to 'skip' by the Multiverse
-
-  # Save all plot configurations alongside the plots
-  save_plot_cfg: true
-
-  # Include dantro's base plot configuration pool
-  use_dantro_base_cfg_pool: true
-
-  # Base plot configuration pools
-  # These specify the base plot configurations that are made available for each
-  # model run, updated and extended in the order specified here and themselves
-  # based on the dantro base config pool.
-  #
-  # In some cases, defining additional pools can be useful, e.g. to generate
-  # publication-ready output without redundantly defining plots or styles.
-  #
-  # This is expected to be a list of 2-tuples in form (name, dict or path).
-  # If the second entry is a string, it may be a format string and it will have
-  # access to `model_name` and the model's `paths` dict.
-  # If there is no file available at the given path, will warn about it and use
-  # an empty pool for that entry.
-  #
-  # There are some special keys, which can be used instead of the 2-tuple:
-  #   `utopya_base`, `framework_base`, `project_base`, `model_base`
-  # These expand to a respective configuration file path, depending on the
-  # framework, project, or model that is being used.
-  base_cfg_pools:
-    - utopya_base
-    - framework_base
-    - project_base
-    - model_base
-
-  # Initialization arguments for all creators
-  shared_creator_init_kwargs:
-    style:
-      figure.figsize: [8., 5.]  # 16:10
-
-  # Can set creator-specific initialization arguments here
-  creator_init_kwargs:
-    pyplot: {}
-    universe: {}
-    multiverse: {}
-
-
-# Parameter Space .............................................................
-# Only entries below this one will be available to the model executable.
-#
-# The content of the `parameter_space` level is parsed by the frontend and then
-# dumped to a file, the path to which is passed to the binary as positional
-# argument.
-parameter_space:
-  # Set a default PRNG seed
-  seed: 42
-
-  # Number of steps to perform
-  num_steps: 3
-
-  # At which step the write_data method should be invoked for the first time
-  write_start: 0
-
-  # Starting from write_start, how frequently write_data should be called
-  write_every: 1
-  # NOTE `write_start` and `write_every` are passed along to sub-models. Every
-  #       sub model can overwrite this entry by adding an entry in their model
-  #       configuration level (analogous to `log_levels`.)
-
-  # Log levels
-  # NOTE The framework may define further levels in here but may also choose
-  #      to ignore these entries altogether. The `model` and `backend` keys
-  #      are those that are accessible from the utopya CLI.
-  log_levels:
-    model: info
-
-    backend: warning
-    # TODO Implement setting this via CLIâ€¦ perhaps even more general?
-    #      Coolest would be: allow frameworks to provide a mapping of each CLI
-    #      debug level to an update dict.
-
-  # Monitoring
-  # How frequently to send a monitoring message to the frontend; note that the
-  # timing needs to be implemented by the model itself
-  monitor_emit_interval: 2.
-
-  # The path to the config file to load
-  # output_path: /abs/path/to/uni<#>/cfg.yml
-  # NOTE This entry is always added by the frontend. Depending on which
-  #      universe is to be simulated, the <#> is set.
-
-  # Below here, the model configuration starts, i.e. the config that is used by
-  # a model instance. It's meant to be nested under the model name itself and
-  # a node of that name will always be added.
-  # <model_name>:
-    # ... more parameters ...
diff --git a/data/Poisson_SinSin2D/config/full_parameter_space.yml b/data/Poisson_SinSin2D/config/full_parameter_space.yml
deleted file mode 100644
index a6b9373..0000000
--- a/data/Poisson_SinSin2D/config/full_parameter_space.yml
+++ /dev/null
@@ -1,42 +0,0 @@
----
-!pspace
-VPINN:
-  NeuralNet:
-    activation_funcs: {0: tanh, 1: tanh, 2: tanh, 3: tanh, 4: None}
-    nodes_per_layer: 20
-    num_layers: 4
-    optimizer: Adam
-  PDE:
-    Burger: {nu: 0}
-    Helmholtz: {k: -2.5}
-    PorousMedium: {m: 2}
-    function: SinSin2D
-    type: Poisson
-  Training: {batch_size: 1, boundary_loss_weight: 1, device: cpu, learning_rate: 0.001,
-    variational_loss_weight: 1, write_time: true}
-  load_data: {copy_data: true, data_dir: null, print_tree: false}
-  predictions_grid:
-    x: {size: 100}
-    y: {size: 100}
-  space:
-    x:
-      extent: [-1, 1]
-      size: 10
-    y:
-      extent: [-1, 1]
-      size: 30
-  test_functions:
-    num_functions:
-      n_x: {size: 10}
-      n_y: {size: 10}
-    type: Legendre
-    weight_function: uniform
-  variational_form: 1
-log_levels: {backend: warning, model: info}
-monitor_emit_interval: 2.0
-num_epochs: 5000
-num_steps: 3
-root_model_name: VPINN
-seed: 42
-write_every: 1
-write_start: 1
diff --git a/data/Poisson_SinSin2D/config/full_parameter_space_info.yml b/data/Poisson_SinSin2D/config/full_parameter_space_info.yml
deleted file mode 100644
index 6d4aadf..0000000
--- a/data/Poisson_SinSin2D/config/full_parameter_space_info.yml
+++ /dev/null
@@ -1,8 +0,0 @@
----
-coupled_dims: []
-dims: []
-num_coupled_dims: 0
-num_dims: 0
-perform_sweep: false
-shape: []
-volume: 0
diff --git a/data/Poisson_SinSin2D/config/git_diff_project.patch b/data/Poisson_SinSin2D/config/git_diff_project.patch
deleted file mode 100644
index 7bed727..0000000
--- a/data/Poisson_SinSin2D/config/git_diff_project.patch
+++ /dev/null
@@ -1,999 +0,0 @@
-diff --git a/include/grid.py b/include/grid.py
-index f2e4841..bd86dab 100644
---- a/include/grid.py
-+++ b/include/grid.py
-@@ -88,7 +88,7 @@ def get_boundary(grid: xarray.DataArray) -> xarray.Dataset:
-
-         return xarray.Dataset(
-             coords=dict(idx=("idx", [0, 1]), variable=("variable", [x, "normals_x"])),
--            data_vars=dict(data=(["idx", "variable"], [[x_0, -1], [x_1, +1]])),
-+            data_vars=dict(boundary_data=(["idx", "variable"], [[x_0, -1], [x_1, +1]])),
-             attrs=grid.attrs,
-         )
-
-@@ -126,7 +126,7 @@ def get_boundary(grid: xarray.DataArray) -> xarray.Dataset:
-                 variable=("variable", [x, y, "normals_x", "normals_y"]),
-             ),
-             data_vars=dict(
--                data=(
-+                boundary_data=(
-                     ["idx", "variable"],
-                     np.stack([x_vals, y_vals, n_vals_x, n_vals_y], axis=1),
-                 )
-@@ -192,7 +192,7 @@ def get_boundary(grid: xarray.DataArray) -> xarray.Dataset:
-                 variable=("variable", [x, y, z, "normals_x", "normals_y", "normals_z"]),
-             ),
-             data_vars=dict(
--                data=(
-+                boundary_data=(
-                     ["idx", "variable"],
-                     np.stack(
-                         [x_vals, y_vals, z_vals, n_vals_x, n_vals_y, n_vals_z], axis=1
-diff --git a/include/integration.py b/include/integration.py
-index 898d072..71d1e7f 100644
---- a/include/integration.py
-+++ b/include/integration.py
-@@ -55,6 +55,10 @@ def integrate_xr(
-     """Integrate a function over the interior of the grid"""
-
-     # TODO use weights
--    return test_function_values.attrs["grid_density"] * (test_function_values * f).isel(
-+    res = test_function_values.attrs["grid_density"] * (test_function_values * f).isel(
-         {val: slice(1, -1) for val in test_function_values.attrs["space_dimensions"]}
-     ).sum(test_function_values.attrs["space_dimensions"])
-+
-+    res.attrs.update(test_function_values.attrs)
-+
-+    return res
-diff --git a/include/neural_net.py b/include/neural_net.py
-index 2a30457..9dc90b6 100644
---- a/include/neural_net.py
-+++ b/include/neural_net.py
-@@ -120,7 +120,7 @@ class NeuralNet(nn.Module):
-                 f"Choose from: [1, 2, 3]"
-             )
-
--        super(NeuralNet, self).__init__()
-+        super().__init__()
-         self.flatten = nn.Flatten()
-
-         self.input_dim = input_size
-diff --git a/model/DataGeneration.py b/model/DataGeneration.py
-index 9492751..e726095 100644
---- a/model/DataGeneration.py
-+++ b/model/DataGeneration.py
-@@ -3,12 +3,13 @@ import logging
- import sys
- from os.path import dirname as up
- from typing import Union
--
- import h5py as h5
- import paramspace
- import xarray as xr
- from dantro._import_tools import import_module_from_path
-
-+import utopya.eval.datamanager
-+
- log = logging.getLogger(__name__)
-
- sys.path.append(up(up(__file__)))
-@@ -22,13 +23,12 @@ base = import_module_from_path(mod_path=up(up(__file__)), mod_str="include")
-
-
- def get_data(
--    load_from_file: str,
-+    load_cfg: dict,
-     space_dict: dict,
-     test_func_dict: dict,
-     *,
-     solution: callable,
-     forcing: callable,
--    var_form: int,
-     boundary_isel: Union[str, tuple] = None,
-     h5file: h5.File,
- ) -> dict:
-@@ -36,7 +36,10 @@ def get_data(
-     """Returns the grid and test function data, either by loading it from a file or generating it.
-     If generated, data is to written to the output folder
-
--    :param load_from_file: the path to the data file. If none, data is automatically generated
-+    :param load_cfg: load configuration, containing the path to the data file. If the path is None, data is
-+         automatically generated. If the ``copy_data`` entry is true, data will be copied and written to the
-+         output directory; however, this is false by default to save disk space.
-+         The configuration also contains further kwargs, passed to the loader.
-     :param space_dict: the dictionary containing the space configuration
-     :param test_func_dict: the dictionary containing the test function configuration
-     :param solution: the explicit solution (to be evaluated on the grid boundary)
-@@ -44,35 +47,98 @@ def get_data(
-     :param var_form: the variational form to use
-     :param boundary_isel: (optional) section of the boundary to use for training. Can either be a string ('lower',
-         'upper', 'left', 'right') or a range.
--    :param h5file: the h5file to write data to
-+    :param h5file: the h5file to write data to. A new group is added to this file.
-+
-     :return: data: a dictionary containing the grid and test function data
-     """
-
-+    # TODO: allow selection of which data to load
-+    # TODO: allow passing Sequences of boundary sections, e.g. ['lower', 'upper']
-+    # TODO: Do not generate separate h5Group?
-+    # TODO: re-writing loaded data not possible
-+
-+    # Collect datasets in a dictionary, passed to the model
-     data = {}
-
--    if load_from_file is not None:
-+    # The directory from which to load data
-+    data_dir = load_cfg.pop("data_dir", None)
-+
-+    if data_dir is not None:
-+
-+        # --------------------------------------------------------------------------------------------------------------
-+        # --- Load data ------------------------------------------------------------------------------------------------
-+        # --------------------------------------------------------------------------------------------------------------
-
-         log.info("   Loading data ...")
--        data = {}
-
--        with h5.File(load_from_file, "r") as f:
--            data["grid"] = f["grid"]
--            data["test_func_values"] = f["test_function_values"]
-+        # If data is loaded, determine whether to copy that data to the new output directory.
-+        # This is false by default to save disk storage
-+        copy_data = load_cfg.pop("copy_data", False)
-
--            if var_form >= 1:
--                data["d1test_func_values"] = f["d1_test_function_values"]
-+        dm = utopya.eval.datamanager.DataManager(data_dir=data_dir, out_dir=False)
-
--            if var_form >= 2:
--                data["d2test_func_values"] = f["d2_test_function_values"]
--                data["d1test_func_values_bd"] = f["d1_test_function_values_boundary"]
-+        dm.load(
-+            "VPINN_data",
-+            loader=load_cfg.pop("loader", "hdf5"),
-+            glob_str=load_cfg.pop("glob_str", "*.h5"),
-+            print_tree=load_cfg.pop("print_tree", False),
-+            **load_cfg,
-+        )
-+
-+        for key in list(dm["VPINN_data"]["data"]["data"].keys()):
-+
-+            # Get the dataset and convert to xr.DataArray
-+            ds = dm["VPINN_data"]["data"]["data"][key]
-+            data[key] = ds.data
-+
-+            # These entries should be xr.Datasets
-+            if key in [
-+                "training_data",
-+                "grid_boundary",
-+                "d1_test_function_values_boundary",
-+            ]:
-+                data[key] = ds.data.to_dataset()
-+
-+            # Manually copy attributes
-+            for attr in [
-+                ("grid_density", lambda x: float(x)),
-+                ("grid_dimension", lambda x: int(x)),
-+                ("space_dimensions", lambda x: [str(_) for _ in x]),
-+                ("test_function_dims", lambda x: [str(_) for _ in x]),
-+            ]:
-+                if attr[0] in ds.attrs.keys():
-+
-+                    data[key].attrs[attr[0]] = attr[1](ds.attrs[attr[0]])
-+
-+        # Rename data
-+        data["training_data"] = data["training_data"].rename(
-+            {"training_data": "boundary_data"}
-+        )
-+
-+        # Stack the test function indices into a single multi-index
-+        for key in [
-+            "test_function_values",
-+            "d1_test_function_values",
-+            "d2_test_function_values",
-+            "d1_test_function_values_boundary",
-+            "f_integrated"
-+        ]:
-+            data[key] = (
-+                data[key]
-+                .stack(tf_idx=data[key].attrs["test_function_dims"])
-+                .transpose("tf_idx", ...)
-+            )
-
-         log.info("   All data loaded")
-
-+        if not copy_data:
-+            return data
-     else:
-
-         # --------------------------------------------------------------------------------------------------------------
-         # --- Generate data --------------------------------------------------------------------------------------------
-         # --------------------------------------------------------------------------------------------------------------
-+
-         if len(space_dict) != len(test_func_dict["num_functions"]):
-             raise ValueError(
-                 f"Space and test function dimensions do not match! "
-@@ -85,7 +151,7 @@ def get_data(
-         grid: xr.DataArray = base.construct_grid(space_dict)
-         boundary: xr.Dataset = base.get_boundary(grid)
-         data["grid"] = grid
--        data["boundary"] = boundary
-+        data["grid_boundary"] = boundary
-         training_boundary = (
-             boundary
-             if boundary_isel is None
-@@ -93,8 +159,8 @@ def get_data(
-         )
-         log.note("   Constructed the grid.")
-
--        # The test functions are only defined on [-1, 1]
--        # TODO Generating two grids can be expensive!
-+        # The test functions are only defined on [-1, 1], so a separate grid is used to generate
-+        # test function values
-         tf_space_dict = paramspace.tools.recursive_replace(
-             copy.deepcopy(space_dict),
-             select_func=lambda d: "extent" in d,
-@@ -105,8 +171,6 @@ def get_data(
-         tf_boundary: xr.Dataset = base.get_boundary(tf_grid)
-
-         log.debug("   Evaluating test functions on grid ...")
--        # The test functions are defined on
--        # TODO the test functions only need to be caluclated on the coordinates
-         test_function_indices = base.construct_grid(
-             test_func_dict["num_functions"], lower=1, dtype=int
-         )
-@@ -115,37 +179,41 @@ def get_data(
-         )
-
-         log.note("   Evaluated the test functions.")
--        data["test_func_values"] = test_function_values.stack(
-+        data["test_function_values"] = test_function_values.stack(
-             tf_idx=test_function_values.attrs["test_function_dims"]
--        )
-+        ).transpose("tf_idx", ...)
-
-         log.debug("   Evaluating test function derivatives on grid ... ")
--        d1test_func_values = base.tf_grid_evaluation(
-+        d1_test_function_values = base.tf_grid_evaluation(
-             tf_grid, test_function_indices, type=test_func_dict["type"], d=1
-         )
-
--        data["d1test_func_values"] = d1test_func_values.stack(
-+        data["d1_test_function_values"] = d1_test_function_values.stack(
-             tf_idx=test_function_values.attrs["test_function_dims"]
--        )
-+        ).transpose("tf_idx", ...)
-
--        d1test_func_values_boundary = base.tf_simple_evaluation(
-+        d1_test_function_values_boundary = base.tf_simple_evaluation(
-             tf_boundary.sel(variable=tf_grid.attrs["space_dimensions"]),
-             test_function_indices,
-             type=test_func_dict["type"],
-             d=1,
-             core_dim="variable",
-         )
--        data["d1test_func_values_boundary"] = d1test_func_values_boundary.stack(
-+        data[
-+            "d1_test_function_values_boundary"
-+        ] = d1_test_function_values_boundary.stack(
-             tf_idx=test_function_values.attrs["test_function_dims"]
--        ).transpose("tf_idx", ...)
-+        ).transpose(
-+            "tf_idx", ...
-+        )
-
-         log.debug("   Evaluating test function second derivatives on grid ... ")
--        d2test_func_values = base.tf_grid_evaluation(
-+        d2_test_function_values = base.tf_grid_evaluation(
-             tf_grid, test_function_indices, type=test_func_dict["type"], d=2
-         )
--        data["d2test_func_values"] = d2test_func_values.stack(
-+        data["d2_test_function_values"] = d2_test_function_values.stack(
-             tf_idx=test_function_values.attrs["test_function_dims"]
--        )
-+        ).transpose("tf_idx", ...)
-
-         log.debug("   Evaluating the external function on the grid ...")
-         f_evaluated: xr.DataArray = xr.apply_ufunc(
-@@ -156,11 +224,11 @@ def get_data(
-         log.debug("   Integrating the function over the grid ...")
-         f_integrated = base.integrate_xr(f_evaluated, test_function_values)
-         data["f_integrated"] = f_integrated.stack(
--            tf_idx=test_function_values.attrs["test_function_dims"]
--        )
-+            tf_idx=f_integrated.attrs["test_function_dims"]
-+        ).transpose("tf_idx", ...)
-
-         log.debug("   Evaluating the solution on the boundary ...")
--        u_boundary: xr.Dataset = xr.concat(
-+        training_data: xr.Dataset = xr.concat(
-             [
-                 training_boundary,
-                 xr.apply_ufunc(
-@@ -172,205 +240,233 @@ def get_data(
-             ],
-             dim="variable",
-         )
--        data["training_data"] = u_boundary
--
--        # --------------------------------------------------------------------------------------------------------------
--        # --- Set up chunked dataset to store the state data in --------------------------------------------------------
--        # --------------------------------------------------------------------------------------------------------------
--
--        data_group = h5file.create_group("data")
--
--        # --------------------------------------------------------------------------------------------------------------
--        # --- Grid -----------------------------------------------------------------------------------------------------
--        # --------------------------------------------------------------------------------------------------------------
--
--        dset_grid = data_group.create_dataset(
--            "grid",
--            list(grid.sizes.values()),
--            maxshape=list(grid.sizes.values()),
--            chunks=True,
--            compression=3,
--        )
--        dset_grid.attrs["dim_names"] = list(grid.sizes)
--
--        # Set attributes
--        for idx in list(grid.sizes):
--            dset_grid.attrs["coords_mode__" + str(idx)] = "values"
--            dset_grid.attrs["coords__" + str(idx)] = grid.coords[idx].data
--
--        dset_grid[
--            :,
--        ] = grid
--
--        # Training data: values of the test function on the boundary
--        dset_boundary = data_group.create_dataset(
--            "grid_boundary",
--            [
--                len(boundary),
--                list(boundary.sizes.values())[0],
--                list(boundary.sizes.values())[1],
--            ],
--            maxshape=[
--                len(boundary),
--                list(boundary.sizes.values())[0],
--                list(boundary.sizes.values())[1],
--            ],
--            chunks=True,
--            compression=3,
--        )
--        dset_boundary.attrs["dim_names"] = ["dim_name__0", "idx", "variable"]
--
--        # Set attributes
--        dset_boundary.attrs["coords_mode__idx"] = "trivial"
--        dset_boundary.attrs["coords_mode__variable"] = "values"
--        dset_boundary.attrs["coords__variable"] = [
--            str(_) for _ in boundary.coords["variable"].data
--        ]
--
--        # Write data
--        dset_boundary[
--            :,
--        ] = boundary.to_array()
--
--        # --------------------------------------------------------------------------------------------------------------
--        # --- Test function values -------------------------------------------------------------------------------------
--        # --------------------------------------------------------------------------------------------------------------
--
--        # Store the test function values
--        dset_test_func_vals = data_group.create_dataset(
--            "test_function_values",
--            test_function_values.shape,
--            maxshape=test_function_values.shape,
--            chunks=True,
--            compression=3,
--        )
--        dset_test_func_vals.attrs["dim_names"] = list(test_function_values.sizes)
--
--        # Store the first derivatives of the test function values
--        dset_d1_test_func_vals = data_group.create_dataset(
--            "d1_test_function_values",
--            d1test_func_values.shape,
--            maxshape=d1test_func_values.shape,
--            chunks=True,
--            compression=3,
--        )
--        dset_d1_test_func_vals.attrs["dim_names"] = list(d1test_func_values.sizes)
--
--        # Store the second derivatives of the test function values
--        dset_d2_test_func_vals = data_group.create_dataset(
--            "d2_test_function_values",
--            d2test_func_values.shape,
--            maxshape=d2test_func_values.shape,
--            chunks=True,
--            compression=3,
--        )
--        dset_d2_test_func_vals.attrs["dim_names"] = list(d2test_func_values.sizes)
--
--        # Set attributes
--        for idx in list(test_function_values.sizes):
--            dset_test_func_vals.attrs["coords_mode__" + str(idx)] = "values"
--            dset_test_func_vals.attrs[
--                "coords__" + str(idx)
--            ] = test_function_values.coords[idx].data
--
--            dset_d1_test_func_vals.attrs["coords_mode__" + str(idx)] = "values"
--            dset_d1_test_func_vals.attrs[
--                "coords__" + str(idx)
--            ] = d1test_func_values.coords[idx].data
--
--            dset_d2_test_func_vals.attrs["coords_mode__" + str(idx)] = "values"
--            dset_d2_test_func_vals.attrs[
--                "coords__" + str(idx)
--            ] = d2test_func_values.coords[idx].data
--
--        # Write the data
--        dset_test_func_vals[
--            :,
--        ] = test_function_values
--        dset_d1_test_func_vals[
--            :,
--        ] = d1test_func_values
--        dset_d2_test_func_vals[
--            :,
--        ] = d2test_func_values
--
--        # --------------------------------------------------------------------------------------------------------------
--        # --- External forcing -----------------------------------------------------------------------------------------
--        # --------------------------------------------------------------------------------------------------------------
--
--        # Store the forcing evaluated on the grid
--        dset_f_evaluated = data_group.create_dataset(
--            "f_evaluated",
--            list(f_evaluated.sizes.values()),
--            maxshape=list(f_evaluated.sizes.values()),
--            chunks=True,
--            compression=3,
--        )
--        dset_f_evaluated.attrs["dim_names"] = list(f_evaluated.sizes)
--
--        # Set the attributes
--        for idx in list(f_evaluated.sizes):
--            dset_f_evaluated.attrs["coords_mode__" + str(idx)] = "values"
--            dset_f_evaluated.attrs["coords__" + str(idx)] = grid.coords[idx].data
--
--        dset_f_evaluated[
--            :,
--        ] = f_evaluated
--
--        # Store the integral of the forcing against the test functions. This dataset is indexed by the
--        # test function indices
--        dset_f_integrated = data_group.create_dataset(
--            "f_integrated",
--            list(f_integrated.sizes.values()),
--            maxshape=list(f_integrated.sizes.values()),
--            chunks=True,
--            compression=3,
--        )
--        dset_f_integrated.attrs["dim_names"] = list(f_integrated.sizes)
--
--        for idx in list(f_integrated.sizes):
--            dset_f_integrated.attrs["coords_mode__" + str(idx)] = "values"
--            dset_f_integrated.attrs["coords__" + str(idx)] = f_integrated.coords[
--                idx
--            ].data
--        dset_f_integrated[
--            :,
--        ] = f_integrated
--
--        # --------------------------------------------------------------------------------------------------------------
--        # --- Exact solution -------------------------------------------------------------------------------------------
--        # --------------------------------------------------------------------------------------------------------------
--
--        # Training data: values of the test function on the boundary
--        dset_u_exact_bd = data_group.create_dataset(
--            "u_exact_boundary",
--            [
--                len(u_boundary),
--                list(u_boundary.sizes.values())[0],
--                list(u_boundary.sizes.values())[1],
--            ],
--            maxshape=[
--                len(u_boundary),
--                list(u_boundary.sizes.values())[0],
--                list(u_boundary.sizes.values())[1],
--            ],
--            chunks=True,
--            compression=3,
--        )
--        dset_u_exact_bd.attrs["dim_names"] = ["dim_name__0", "idx", "variable"]
--
--        # Set attributes
--        dset_u_exact_bd.attrs["coords_mode__idx"] = "trivial"
--        dset_u_exact_bd.attrs["coords_mode__variable"] = "values"
--        dset_u_exact_bd.attrs["coords__variable"] = [
--            str(_) for _ in u_boundary.coords["variable"].data
--        ]
--
--        # Write data
--        dset_u_exact_bd[
--            :,
--        ] = u_boundary.to_array()
--
--        log.info("   All data generated and saved.")
-+        data["training_data"] = training_data
-+
-+        log.info("   Generated data.")
-+    # ------------------------------------------------------------------------------------------------------------------
-+    # --- Set up chunked dataset to store the state data in ------------------------------------------------------------
-+    # ------------------------------------------------------------------------------------------------------------------
-+
-+    log.info("   Saving data ... ")
-+    data_group = h5file.create_group("data")
-+
-+    # ------------------------------------------------------------------------------------------------------------------
-+    # --- Grid and grid boundary ---------------------------------------------------------------------------------------
-+    # ------------------------------------------------------------------------------------------------------------------
-+
-+    # Grid
-+    grid = data["grid"]
-+    dset_grid = data_group.create_dataset(
-+        "grid",
-+        list(grid.sizes.values()),
-+        maxshape=list(grid.sizes.values()),
-+        chunks=True,
-+        compression=3,
-+    )
-+    dset_grid.attrs["dim_names"] = [str(_) for _ in list(grid.sizes)]
-+
-+    # Set attributes
-+    for idx in list(grid.sizes):
-+        dset_grid.attrs["coords_mode__" + str(idx)] = "values"
-+        dset_grid.attrs["coords__" + str(idx)] = grid.coords[idx].data
-+    dset_grid.attrs.update(grid.attrs)
-+
-+    # Write data
-+    dset_grid[
-+        :,
-+    ] = grid
-+
-+    # Grid boundary
-+    dset_boundary = data_group.create_dataset(
-+        "grid_boundary",
-+        list(data["grid_boundary"].sizes.values()),
-+        maxshape=list(data["grid_boundary"].sizes.values()),
-+        chunks=True,
-+        compression=3,
-+    )
-+    dset_boundary.attrs["dim_names"] = ["idx", "variable"]
-+
-+    # Set attributes
-+    dset_boundary.attrs["coords_mode__idx"] = "trivial"
-+    dset_boundary.attrs["coords_mode__variable"] = "values"
-+    dset_boundary.attrs["coords__variable"] = [
-+        str(_) for _ in data["grid_boundary"].coords["variable"].data
-+    ]
-+    dset_boundary.attrs.update(data["grid_boundary"].attrs)
-+
-+    # Write data
-+    dset_boundary[
-+        :,
-+    ] = data["grid_boundary"].to_array()
-+
-+    # ------------------------------------------------------------------------------------------------------------------
-+    # --- Test function values -----------------------------------------------------------------------------------------
-+    # ------------------------------------------------------------------------------------------------------------------
-+
-+    # Test function values
-+    test_function_values = data["test_function_values"].unstack()
-+    dset_test_function_values = data_group.create_dataset(
-+        "test_function_values",
-+        test_function_values.shape,
-+        maxshape=test_function_values.shape,
-+        chunks=True,
-+        compression=3,
-+    )
-+    dset_test_function_values.attrs["dim_names"] = [str(_) for _ in list(test_function_values.sizes)]
-+
-+    # First derivatives of the test function values
-+    d1_test_function_values = data["d1_test_function_values"].unstack()
-+    dset_d1_test_function_values = data_group.create_dataset(
-+        "d1_test_function_values",
-+        d1_test_function_values.shape,
-+        maxshape=d1_test_function_values.shape,
-+        chunks=True,
-+        compression=3,
-+    )
-+    dset_d1_test_function_values.attrs["dim_names"] = [str(_) for _ in list(
-+        d1_test_function_values.sizes
-+    )]
-+
-+    # Second derivatives of the test function values
-+    d2_test_function_values = data["d2_test_function_values"].unstack()
-+    dset_d2_test_function_values = data_group.create_dataset(
-+        "d2_test_function_values",
-+        d2_test_function_values.shape,
-+        maxshape=d2_test_function_values.shape,
-+        chunks=True,
-+        compression=3,
-+    )
-+    dset_d2_test_function_values.attrs["dim_names"] = [str(_) for _ in list(
-+        d2_test_function_values.sizes
-+    )]
-+
-+    # First derivatives of the test function values on the boundary
-+    d1_test_function_values_boundary = data["d1_test_function_values_boundary"].unstack().to_array()
-+    dset_d1_test_function_values_boundary = data_group.create_dataset(
-+        "d1_test_function_values_boundary",
-+        list(d1_test_function_values_boundary.shape),
-+        maxshape=list(d1_test_function_values_boundary.shape),
-+        chunks=True,
-+        compression=3,
-+    )
-+    dset_d1_test_function_values_boundary.attrs["dim_names"] = [str(_) for _ in list(
-+        d1_test_function_values_boundary.sizes
-+    )]
-+
-+    # Set attributes
-+    for idx in list(test_function_values.sizes):
-+        dset_test_function_values.attrs["coords_mode__" + str(idx)] = "values"
-+        dset_test_function_values.attrs[
-+            "coords__" + str(idx)
-+        ] = test_function_values.coords[idx].data
-+
-+        dset_d1_test_function_values.attrs["coords_mode__" + str(idx)] = "values"
-+        dset_d1_test_function_values.attrs[
-+            "coords__" + str(idx)
-+        ] = d1_test_function_values.coords[idx].data
-+
-+        dset_d2_test_function_values.attrs["coords_mode__" + str(idx)] = "values"
-+        dset_d2_test_function_values.attrs[
-+            "coords__" + str(idx)
-+        ] = d2_test_function_values.coords[idx].data
-+    dset_test_function_values.attrs.update(test_function_values.attrs)
-+    dset_d1_test_function_values.attrs.update(d1_test_function_values.attrs)
-+    dset_d2_test_function_values.attrs.update(d2_test_function_values.attrs)
-+    dset_d1_test_function_values_boundary.attrs.update(
-+        d1_test_function_values_boundary.attrs
-+    )
-+
-+    # Write the data
-+    dset_test_function_values[
-+        :,
-+    ] = test_function_values
-+    dset_d1_test_function_values[
-+        :,
-+    ] = d1_test_function_values
-+    dset_d2_test_function_values[
-+        :,
-+    ] = d2_test_function_values
-+    dset_d1_test_function_values_boundary[
-+        :,
-+    ] = d1_test_function_values_boundary
-+
-+    # ------------------------------------------------------------------------------------------------------------------
-+    # --- External forcing ---------------------------------------------------------------------------------------------
-+    # ------------------------------------------------------------------------------------------------------------------
-+
-+    # External function evaluated on the grid
-+    f_evaluated = data["f_evaluated"]
-+    dset_f_evaluated = data_group.create_dataset(
-+        "f_evaluated",
-+        list(f_evaluated.sizes.values()),
-+        maxshape=list(f_evaluated.sizes.values()),
-+        chunks=True,
-+        compression=3,
-+    )
-+    dset_f_evaluated.attrs["dim_names"] = [str(_) for _ in list(f_evaluated.sizes)]
-+
-+    # Set the attributes
-+    for idx in list(f_evaluated.sizes):
-+        dset_f_evaluated.attrs["coords_mode__" + str(idx)] = "values"
-+        dset_f_evaluated.attrs["coords__" + str(idx)] = grid.coords[idx].data
-+    dset_f_evaluated.attrs.update(f_evaluated.attrs)
-+
-+    # Write the data
-+    dset_f_evaluated[
-+        :,
-+    ] = f_evaluated
-+
-+    # Integral of the forcing against the test functions.
-+    # This dataset is indexed by the test function indices
-+    f_integrated = data["f_integrated"].unstack()
-+    dset_f_integrated = data_group.create_dataset(
-+        "f_integrated",
-+        list(f_integrated.sizes.values()),
-+        maxshape=list(f_integrated.sizes.values()),
-+        chunks=True,
-+        compression=3,
-+    )
-+    dset_f_integrated.attrs["dim_names"] = [str(_) for _ in list(f_integrated.sizes)]
-+
-+    # Set attributes
-+    for idx in list(f_integrated.sizes):
-+        dset_f_integrated.attrs["coords_mode__" + str(idx)] = "values"
-+        dset_f_integrated.attrs["coords__" + str(idx)] = f_integrated.coords[idx].data
-+    dset_f_integrated.attrs.update(f_integrated.attrs)
-+
-+    # Write data
-+    dset_f_integrated[
-+        :,
-+    ] = f_integrated
-+
-+    # ------------------------------------------------------------------------------------------------------------------
-+    # --- Boundary training data ---------------------------------------------------------------------------------------
-+    # ------------------------------------------------------------------------------------------------------------------
-+
-+    # Training data: values of the test function on the boundary
-+    training_data = data["training_data"]
-+    dset_training_data = data_group.create_dataset(
-+        "training_data",
-+        list(training_data.sizes.values()),
-+        maxshape=list(training_data.sizes.values()),
-+        chunks=True,
-+        compression=3,
-+    )
-+    dset_training_data.attrs["dim_names"] = [str(_) for _ in list(training_data.sizes)]
-+    dset_training_data.attrs.update(training_data.attrs)
-+
-+    # Set attributes
-+    dset_training_data.attrs["coords_mode__idx"] = "trivial"
-+    dset_training_data.attrs["coords_mode__variable"] = "values"
-+    dset_training_data.attrs["coords__variable"] = [
-+        str(_) for _ in training_data.coords["variable"].data
-+    ]
-+
-+    # Write data
-+    dset_training_data[
-+        :,
-+    ] = training_data.to_array()
-+
-+    log.info("   All data saved.")
-
-     return data
-diff --git a/model/VPINN_cfg.yml b/model/VPINN_cfg.yml
-index f352739..5de9815 100644
---- a/model/VPINN_cfg.yml
-+++ b/model/VPINN_cfg.yml
-@@ -1,7 +1,9 @@
- ---
- # Data loading .........................................................................................................
--
--load_from_dir: ~
-+load_data:
-+  data_dir: ~ # folder containing the .h5 file
-+  copy_data: False # Whether to copy the loaded data to the new directory
-+  print_tree: False # other parameters here will be passed to utopya.eval.DataManager.load
-
- # Domain settings ......................................................................................................
- space:
-@@ -11,14 +13,17 @@ space:
-
- # PDE parameters .......................................................................................................
- PDE:
-+  # Type of equation
-   type: !param
-     default: Burger
-     dtype: str
-     is_any_of: [Poisson, Burger]
-+  # External function example
-   function: !param
-     default: Tanh
-     dtype: str
-     is_any_of: [Tanh, Tanh2D, SinSin2D, Burger1+1D, DoubleGauss1D, CubedRoot, PorousMedium]
-+  # Scalar parameters for the equations
-   PorousMedium:
-     m: 2
-   Helmholtz:
-diff --git a/model/cfgs/Poisson1D/run.yml b/model/cfgs/Poisson1D/run.yml
-index b224988..85e7b45 100644
---- a/model/cfgs/Poisson1D/run.yml
-+++ b/model/cfgs/Poisson1D/run.yml
-@@ -2,12 +2,15 @@
- paths:
-   model_note: Poisson1D
- parameter_space:
--  num_epochs: 5000
-+  num_epochs: 1
-   VPINN:
-+    load_data:
-+      data_dir: /Users/thomasgaskin/utopya_output/VPINN/221014-103039_Poisson1D/data/uni0
-+      copy_data: True
-     space:
-       x:
-         extent: [-1, 1 ]
--        size: 60
-+        size: 6
-     PDE:
-       function: Tanh
-       type: Poisson
-@@ -15,7 +18,7 @@ parameter_space:
-       type: Legendre
-       num_functions:
-         n_x:
--          size: 80
-+          size: 8
-       weight_function: uniform
-     variational_form: 0
-     NeuralNet:
-@@ -28,7 +31,7 @@ parameter_space:
-         3: tanh
-         4: linear
-     Training:
--      device: mps
-+      device: cpu
-       learning_rate: 0.001
-       variational_loss_weight: 20
-     predictions_grid:
-diff --git a/model/cfgs/Poisson2D/eval.yml b/model/cfgs/Poisson2D/eval.yml
-index 6bed6ee..29524f9 100644
---- a/model/cfgs/Poisson2D/eval.yml
-+++ b/model/cfgs/Poisson2D/eval.yml
-@@ -45,7 +45,7 @@ grid_boundary:
- boundary_conditions:
-   based_on: grid_boundary2d
-   select:
--    vals: data/u_exact_boundary
-+    vals: data/training_data
-   transform:
-     - .isel: [ !dag_prev , { variable: 0 } ]
-       kwargs:
-@@ -115,7 +115,7 @@ d1test_functions_norm:
-     data:
-       path: data/d1_test_function_values
-       transform:
--        - .isel: [!dag_prev , {n_x: !range [3], n_y: !range [3], idx: 1}]
-+        - .isel: [!dag_prev , {n_x: !range [3], n_y: !range [3]}]
-         - squared: [ !dag_prev ,  ]
-         - .sum: [!dag_prev , 'idx']
-   row: n_x
-diff --git a/model/cfgs/Poisson2D/run.yml b/model/cfgs/Poisson2D/run.yml
-index fc82cb1..c114d1f 100644
---- a/model/cfgs/Poisson2D/run.yml
-+++ b/model/cfgs/Poisson2D/run.yml
-@@ -4,6 +4,16 @@ paths:
- parameter_space:
-   num_epochs: 5000
-   VPINN:
-+    load_data:
-+
-+      # Set to '~' (None) to use the settings below to generate data
-+      data_dir: ~ # /Users/thomasgaskin/utopya_output/VPINN/221014-103238_Poisson2D/data/uni0
-+
-+      # copies to the data to the new output folder. Set to 'false' to conserve disk space.
-+      # This will cause any plots requiring that data to no longer function
-+      copy_data: true
-+
-+    # Settings for the grid space
-     space:
-       x:
-         extent: [ -1, 1]
-@@ -11,9 +21,13 @@ parameter_space:
-       y:
-         extent: [-1, 1]
-         size: 30
-+
-+    # PDE settings
-     PDE:
-       function: SinSin2D
-       type: Poisson
-+
-+    # Test function settings
-     test_functions:
-       type: Legendre
-       num_functions:
-@@ -23,6 +37,8 @@ parameter_space:
-           size: 10
-       weight_function: uniform
-     variational_form: 1
-+
-+    # Neural net architecture
-     NeuralNet:
-       num_layers: 4
-       nodes_per_layer: 20
-@@ -32,6 +48,8 @@ parameter_space:
-         2: tanh
-         3: tanh
-         4: None
-+
-+    # Settings for the prediction resolution: updates the 'space' dictionary
-     predictions_grid:
-       x:
-         size: 100
-diff --git a/model/run.py b/model/run.py
-index ead5562..3bb41ac 100755
---- a/model/run.py
-+++ b/model/run.py
-@@ -44,10 +44,10 @@ class VPINN:
-         grid: xr.DataArray,
-         training_data: xr.Dataset = None,
-         f_integrated: xr.DataArray,
--        test_func_values: xr.DataArray,
--        d1test_func_values: xr.DataArray = None,
--        d2test_func_values: xr.DataArray = None,
--        d1test_func_values_boundary: xr.DataArray = None,
-+        test_function_values: xr.DataArray,
-+        d1_test_function_values: xr.DataArray = None,
-+        d2_test_function_values: xr.DataArray = None,
-+        d1_test_function_values_boundary: xr.DataArray = None,
-         weight_function: callable = lambda x: 1,
-         **__,
-     ):
-@@ -144,7 +144,7 @@ class VPINN:
-                     torch.from_numpy(
-                         training_data.sel(
-                             variable=["normals_" + str(dim)], drop=True
--                        ).data.to_numpy()
-+                        ).boundary_data.to_numpy()
-                     ).float()
-                 )
-
-@@ -216,7 +216,7 @@ class VPINN:
-             torch.from_numpy(
-                 training_data.sel(
-                     variable=grid.attrs["space_dimensions"], drop=True
--                ).data.to_numpy()
-+                ).boundary_data.to_numpy()
-             )
-             .float()
-             .to(device)
-@@ -231,7 +231,7 @@ class VPINN:
-         # Training data (boundary conditions)
-         self.training_data: torch.Tensor = (
-             torch.from_numpy(
--                training_data.sel(variable=["u"], drop=True).data.to_numpy()
-+                training_data.sel(variable=["u"], drop=True).boundary_data.to_numpy()
-             )
-             .float()
-             .to(device)
-@@ -244,20 +244,20 @@ class VPINN:
-
-         # Test function values on the grid interior, indexed by their (multi-)index and grid coordinate
-         self.test_func_values: torch.Tensor = _tf_to_tensor(
--            test_func_values.transpose("tf_idx", ...)
-+            test_function_values
-         )
-
-         self.d1test_func_values: Union[None, torch.Tensor] = _dtf_to_tensor(
--            d1test_func_values.transpose("tf_idx", ...)
-+            d1_test_function_values
-         )
-
-         self.d2test_func_values: Union[None, xr.DataArray] = _dtf_to_tensor(
--            d2test_func_values.transpose("tf_idx", ...)
-+            d2_test_function_values
-         )
-
-         self.d1test_func_values_boundary: torch.Tensor = (
-             torch.from_numpy(
--                d1test_func_values_boundary.transpose("tf_idx", ...)
-+                d1_test_function_values_boundary
-                 .to_array()
-                 .squeeze()
-                 .data
-@@ -271,7 +271,7 @@ class VPINN:
-                 torch.stack(
-                     [
-                         weight_function(np.array(idx))
--                        for idx in test_func_values.coords["tf_idx"].data
-+                        for idx in test_function_values.coords["tf_idx"].data
-                     ]
-                 ),
-                 (-1, 1),
-@@ -359,7 +359,7 @@ if __name__ == "__main__":
-
-     log.note("   Preparing model run ...")
-     log.note(f"   Loading config file:\n        {cfg_file_path}")
--    with open(cfg_file_path, "r") as cfg_file:
-+    with open(cfg_file_path) as cfg_file:
-         cfg = yaml.load(cfg_file, Loader=yaml.Loader)
-     model_name = cfg.get("root_model_name", "VPINN")
-     log.note(f"   Model name:  {model_name}")
-@@ -418,12 +418,11 @@ if __name__ == "__main__":
-     # Get the data: grid, test function data, and training data. This is loaded from a file,
-     # if provided, else synthetically generated
-     data: dict = this.get_data(
--        model_cfg.get("load_from_file", None),
-+        model_cfg.get("load_data", {}),
-         model_cfg["space"],
-         test_func_dict,
-         solution=this.EXAMPLES[model_cfg["PDE"]["function"]]["u"],
-         forcing=this.EXAMPLES[model_cfg["PDE"]["function"]]["f"],
--        var_form=model_cfg["variational_form"],
-         boundary_isel=model_cfg["Training"].get("boundary", None),
-         h5file=h5file,
-     )
diff --git a/data/Poisson_SinSin2D/config/git_info_project.yml b/data/Poisson_SinSin2D/config/git_info_project.yml
deleted file mode 100644
index 27fd8dc..0000000
--- a/data/Poisson_SinSin2D/config/git_info_project.yml
+++ /dev/null
@@ -1,22 +0,0 @@
----
-dirty: true
-git_diff: /Users/thomasgaskin/utopya_output/VPINN/221014-103319_Poisson2D/config/git_diff_project.patch
-git_status:
-- [M, include/grid.py]
-- ['', M include/integration.py]
-- ['', M include/neural_net.py]
-- ['', M model/DataGeneration.py]
-- ['', M model/VPINN_cfg.yml]
-- ['', M model/cfgs/Poisson1D/run.yml]
-- ['', M model/cfgs/Poisson2D/eval.yml]
-- ['', M model/cfgs/Poisson2D/run.yml]
-- ['', M model/run.py]
-- [??, data/]
-have_git_repo: true
-latest_commit: {author: Thomas Gaskin <tgaskin@live.co.uk>, author_date: '2022-10-12
-    13:11:34', commit: d1769ab524d9f4dfb552652f7fe4885f038229f2, gitdir: /Users/thomasgaskin/VPINNs/.git,
-  message: "Merge pull request #5 from ThGaskin/enable_gpu_training\nEnable GPU training",
-  parent: dfe54a9a5c7d176339d67037bd2f2411aa51d6f4, refs: enable_loading_from_file,
-  tree: 6669fb012951a183d08809bafa9d9b48b8500ecf}
-project_base_dir: /Users/thomasgaskin/VPINNs
-project_name: VPINNS
diff --git a/data/Poisson_SinSin2D/config/meta_cfg.yml b/data/Poisson_SinSin2D/config/meta_cfg.yml
deleted file mode 100644
index ae9f0e0..0000000
--- a/data/Poisson_SinSin2D/config/meta_cfg.yml
+++ /dev/null
@@ -1,180 +0,0 @@
----
-backups: {backup_cfg_files: true, backup_executable: false, include_git_info: true}
-cluster_mode: false
-cluster_params:
-  additional_run_dir_fstrs: ['job{job_id:}']
-  env: null
-  env_var_names:
-    slurm: {cluster_name: SLURM_CLUSTER_NAME, custom_out_dir: UTOPIA_CLUSTER_MODE_OUT_DIR,
-      job_account: SLURM_JOB_ACCOUNT, job_id: SLURM_JOB_ID, job_name: SLURM_JOB_NAME,
-      node_list: SLURM_JOB_NODELIST, node_name: SLURMD_NODENAME, num_nodes: SLURM_JOB_NUM_NODES,
-      num_procs: SLURM_CPUS_ON_NODE, timestamp: RUN_TIMESTAMP}
-  manager: slurm
-  node_list_parser_params: {slurm: condensed}
-data_manager:
-  create_groups:
-  - {Cls: MultiverseGroup, path: multiverse}
-  default_tree_cache_path: data/.tree_cache.d3
-  load_cfg:
-    cfg:
-      glob_str: config/*.yml
-      ignore: [config/parameter_space.yml, config/parameter_space_info.yml, config/full_parameter_space.yml,
-        config/full_parameter_space_info.yml, config/git_info_project.yml, config/git_info_framework.yml]
-      loader: yaml
-      path_regex: config/(\w+)_cfg.yml
-      required: true
-      target_path: cfg/{match:}
-    data:
-      enable_mapping: true
-      glob_str: data/uni*/data.h5
-      loader: hdf5_proxy
-      parallel: {enabled: true, min_files: 5, min_total_size: 104857600, processes: null}
-      path_regex: data/uni(\d+)/data.h5
-      required: true
-      target_path: multiverse/{match:}/data
-    pspace: {glob_str: config/parameter_space.yml, load_as_attr: true, loader: yaml_to_object,
-      required: true, target_path: multiverse, unpack_data: true}
-    uni_cfg:
-      glob_str: data/uni*/config.yml
-      loader: yaml
-      parallel: {enabled: true, min_files: 1000, min_total_size: 1048576}
-      path_regex: data/uni(\d+)/config.yml
-      required: true
-      target_path: multiverse/{match:}/cfg
-  out_dir: eval/{timestamp:}
-debug_level: 0
-executable_control: {run_from_tmpdir: false}
-parameter_space: !pspace
-  VPINN:
-    NeuralNet:
-      activation_funcs: {0: tanh, 1: tanh, 2: tanh, 3: tanh, 4: None}
-      nodes_per_layer: 20
-      num_layers: 4
-      optimizer: Adam
-    PDE:
-      Burger: {nu: 0}
-      Helmholtz: {k: -2.5}
-      PorousMedium: {m: 2}
-      function: SinSin2D
-      type: Poisson
-    Training: {batch_size: 1, boundary_loss_weight: 1, device: cpu, learning_rate: 0.001,
-      variational_loss_weight: 1, write_time: true}
-    load_data: {copy_data: true, data_dir: null, print_tree: false}
-    predictions_grid:
-      x: {size: 100}
-      y: {size: 100}
-    space:
-      x:
-        extent: [-1, 1]
-        size: 10
-      y:
-        extent: [-1, 1]
-        size: 30
-    test_functions:
-      num_functions:
-        n_x: {size: 10}
-        n_y: {size: 10}
-      type: Legendre
-      weight_function: uniform
-    variational_form: 1
-  log_levels: {backend: warning, model: info}
-  monitor_emit_interval: 2.0
-  num_epochs: 5000
-  num_steps: 3
-  root_model_name: VPINN
-  seed: 42
-  write_every: 1
-  write_start: 1
-parameters_to_validate:
-  [VPINN, NeuralNet, nodes_per_layer]: !is-positive-int 20
-  [VPINN, NeuralNet, num_layers]: !is-positive-int 4
-  [VPINN, NeuralNet, optimizer]: !param
-    default: Adam
-    is_any_of: [Adagrad, Adam, AdamW, SparseAdam, Adamax, ASGD, LBFGS, NAdam, RAdam,
-      RMSprop, Rprop, SGD]
-  [VPINN, PDE, Burger, nu]: !is-positive-or-zero 0
-  [VPINN, PDE, function]: !param
-    default: Tanh
-    dtype: <U0
-    is_any_of: [Tanh, Tanh2D, SinSin2D, Burger1+1D, DoubleGauss1D, CubedRoot, PorousMedium]
-  [VPINN, PDE, type]: !param
-    default: Burger
-    dtype: <U0
-    is_any_of: [Poisson, Burger]
-  [VPINN, test_functions, type]: !param
-    default: Legendre
-    dtype: <U0
-    is_any_of: [Legendre, Chebyshev, Sine]
-  [VPINN, test_functions, weight_function]: !param
-    default: exponential
-    is_any_of: [uniform, exponential]
-  [VPINN, variational_form]: !param
-    default: 1
-    is_any_of: [0, 1, 2]
-paths: {model_note: Poisson2D, out_dir: ~/utopya_output}
-perform_sweep: false
-perform_validation: true
-plot_manager:
-  base_cfg_pools: [utopya_base, framework_base, project_base, model_base]
-  cfg_exists_action: raise
-  creator_init_kwargs:
-    multiverse: {}
-    pyplot: {}
-    universe: {}
-  out_dir: ''
-  raise_exc: false
-  save_plot_cfg: true
-  shared_creator_init_kwargs:
-    style:
-      figure.figsize: [8.0, 5.0]
-  use_dantro_base_cfg_pool: true
-reporter:
-  report_formats:
-    progress_bar:
-      info_fstr: '{total_progress:>5.1f}% '
-      min_report_intv: 0.5
-      num_cols: adaptive
-      parser: progress_bar
-      show_times: true
-      times_fstr: '| {elapsed:>7s} elapsed | ~{est_left:>7s} left '
-      times_fstr_final: '| finished in {elapsed:} '
-      times_kwargs: {mode: from_buffer, progress_buffer_size: 90}
-      write_to: stdout_noreturn
-    report_file:
-      min_num: 4
-      min_report_intv: 10
-      parser: report
-      show_individual_runtimes: true
-      task_label_plural: universes
-      task_label_singular: universe
-      write_to:
-        file: {path: _report.txt}
-    sweep_info:
-      fstr: "Sweeping over the following parameter space:\n\n{sweep_info:}"
-      parser: pspace_info
-      write_to:
-        file: {path: _sweep_info.txt, skip_if_empty: true}
-        log: {lvl: 18, skip_if_empty: true}
-run_kwargs: {stop_conditions: null, timeout: null}
-worker_kwargs:
-  forward_raw: true
-  forward_streams: in_single_run
-  popen_kwargs: {encoding: utf8}
-  save_streams: true
-  streams_log_lvl: null
-worker_manager:
-  interrupt_params: {exit: false, grace_period: 5.0, send_signal: SIGINT}
-  lines_per_poll: 20
-  nonzero_exit_handling: raise
-  num_workers: auto
-  periodic_task_callback: null
-  poll_delay: 0.05
-  rf_spec:
-    after_abort: [progress_bar, report_file]
-    after_work: [progress_bar, report_file]
-    before_working: [sweep_info]
-    monitor_updated: [progress_bar]
-    task_finished: [progress_bar, report_file]
-    task_spawned: [progress_bar]
-    while_working: [progress_bar]
-  save_streams_on: [monitor_updated]
diff --git a/data/Poisson_SinSin2D/config/model_cfg.yml b/data/Poisson_SinSin2D/config/model_cfg.yml
deleted file mode 100644
index 5de9815..0000000
--- a/data/Poisson_SinSin2D/config/model_cfg.yml
+++ /dev/null
@@ -1,71 +0,0 @@
----
-# Data loading .........................................................................................................
-load_data:
-  data_dir: ~ # folder containing the .h5 file
-  copy_data: False # Whether to copy the loaded data to the new directory
-  print_tree: False # other parameters here will be passed to utopya.eval.DataManager.load
-
-# Domain settings ......................................................................................................
-space:
-  x:
-    extent: [-1, 1]
-    size: 20
-
-# PDE parameters .......................................................................................................
-PDE:
-  # Type of equation
-  type: !param
-    default: Burger
-    dtype: str
-    is_any_of: [Poisson, Burger]
-  # External function example
-  function: !param
-    default: Tanh
-    dtype: str
-    is_any_of: [Tanh, Tanh2D, SinSin2D, Burger1+1D, DoubleGauss1D, CubedRoot, PorousMedium]
-  # Scalar parameters for the equations
-  PorousMedium:
-    m: 2
-  Helmholtz:
-    k: -2.5
-  Burger:
-    nu: !is-positive-or-zero 0
-
-# The variational form to use; can be 0, 1, or 2
-variational_form: !param
-  default: 1
-  is_any_of: [0, 1, 2]
-
-# Test function settings ...............................................................................................
-test_functions:
-  num_functions:
-    n_x:
-      size: 3
-  type: !param
-    default: Legendre
-    dtype: str
-    is_any_of: [Legendre, Chebyshev, Sine]
-
-  # The type of weighting to use
-  weight_function: !param
-    default: exponential
-    is_any_of: [uniform, exponential]
-
-
-# Neural net architecture ..............................................................................................
-NeuralNet:
-  num_layers: !is-positive-int 4
-  nodes_per_layer: !is-positive-int 20
-  activation_funcs: tanh
-  optimizer: !param
-    default: Adam
-    is_any_of: [Adagrad, Adam, AdamW, SparseAdam, Adamax, ASGD, LBFGS, NAdam, RAdam, RMSprop, Rprop, SGD]
-
-# Training settings ....................................................................................................
-Training:
-  device: cpu
-  batch_size: 1
-  learning_rate: 0.001
-  boundary_loss_weight: 1
-  variational_loss_weight: 1
-  write_time: True
diff --git a/data/Poisson_SinSin2D/config/parameter_space.yml b/data/Poisson_SinSin2D/config/parameter_space.yml
deleted file mode 100644
index a6b9373..0000000
--- a/data/Poisson_SinSin2D/config/parameter_space.yml
+++ /dev/null
@@ -1,42 +0,0 @@
----
-!pspace
-VPINN:
-  NeuralNet:
-    activation_funcs: {0: tanh, 1: tanh, 2: tanh, 3: tanh, 4: None}
-    nodes_per_layer: 20
-    num_layers: 4
-    optimizer: Adam
-  PDE:
-    Burger: {nu: 0}
-    Helmholtz: {k: -2.5}
-    PorousMedium: {m: 2}
-    function: SinSin2D
-    type: Poisson
-  Training: {batch_size: 1, boundary_loss_weight: 1, device: cpu, learning_rate: 0.001,
-    variational_loss_weight: 1, write_time: true}
-  load_data: {copy_data: true, data_dir: null, print_tree: false}
-  predictions_grid:
-    x: {size: 100}
-    y: {size: 100}
-  space:
-    x:
-      extent: [-1, 1]
-      size: 10
-    y:
-      extent: [-1, 1]
-      size: 30
-  test_functions:
-    num_functions:
-      n_x: {size: 10}
-      n_y: {size: 10}
-    type: Legendre
-    weight_function: uniform
-  variational_form: 1
-log_levels: {backend: warning, model: info}
-monitor_emit_interval: 2.0
-num_epochs: 5000
-num_steps: 3
-root_model_name: VPINN
-seed: 42
-write_every: 1
-write_start: 1
diff --git a/data/Poisson_SinSin2D/config/parameter_space_info.yml b/data/Poisson_SinSin2D/config/parameter_space_info.yml
deleted file mode 100644
index 6d4aadf..0000000
--- a/data/Poisson_SinSin2D/config/parameter_space_info.yml
+++ /dev/null
@@ -1,8 +0,0 @@
----
-coupled_dims: []
-dims: []
-num_coupled_dims: 0
-num_dims: 0
-perform_sweep: false
-shape: []
-volume: 0
diff --git a/data/Poisson_SinSin2D/config/run_cfg.yml b/data/Poisson_SinSin2D/config/run_cfg.yml
deleted file mode 100644
index c114d1f..0000000
--- a/data/Poisson_SinSin2D/config/run_cfg.yml
+++ /dev/null
@@ -1,57 +0,0 @@
----
-paths:
-  model_note: Poisson2D
-parameter_space:
-  num_epochs: 5000
-  VPINN:
-    load_data:
-
-      # Set to '~' (None) to use the settings below to generate data
-      data_dir: ~ # /Users/thomasgaskin/utopya_output/VPINN/221014-103238_Poisson2D/data/uni0
-
-      # copies to the data to the new output folder. Set to 'false' to conserve disk space.
-      # This will cause any plots requiring that data to no longer function
-      copy_data: true
-
-    # Settings for the grid space
-    space:
-      x:
-        extent: [ -1, 1]
-        size: 10
-      y:
-        extent: [-1, 1]
-        size: 30
-
-    # PDE settings
-    PDE:
-      function: SinSin2D
-      type: Poisson
-
-    # Test function settings
-    test_functions:
-      type: Legendre
-      num_functions:
-        n_x:
-          size: 10
-        n_y:
-          size: 10
-      weight_function: uniform
-    variational_form: 1
-
-    # Neural net architecture
-    NeuralNet:
-      num_layers: 4
-      nodes_per_layer: 20
-      activation_funcs:
-        0: tanh
-        1: tanh
-        2: tanh
-        3: tanh
-        4: None
-
-    # Settings for the prediction resolution: updates the 'space' dictionary
-    predictions_grid:
-      x:
-        size: 100
-      y:
-        size: 100
diff --git a/data/Poisson_SinSin2D/config/update_cfg.yml b/data/Poisson_SinSin2D/config/update_cfg.yml
deleted file mode 100644
index f7e0d94..0000000
--- a/data/Poisson_SinSin2D/config/update_cfg.yml
+++ /dev/null
@@ -1,2 +0,0 @@
----
-{debug_level: 0}
diff --git a/data/Poisson_SinSin2D/data/uni0/config.yml b/data/Poisson_SinSin2D/data/uni0/config.yml
deleted file mode 100644
index fde65c8..0000000
--- a/data/Poisson_SinSin2D/data/uni0/config.yml
+++ /dev/null
@@ -1,43 +0,0 @@
----
-VPINN:
-  NeuralNet:
-    activation_funcs: {0: tanh, 1: tanh, 2: tanh, 3: tanh, 4: None}
-    nodes_per_layer: 20
-    num_layers: 4
-    optimizer: Adam
-  PDE:
-    Burger: {nu: 0}
-    Helmholtz: {k: -2.5}
-    PorousMedium: {m: 2}
-    function: SinSin2D
-    type: Poisson
-  Training: {batch_size: 1, boundary_loss_weight: 1, device: cpu, learning_rate: 0.001,
-    variational_loss_weight: 1, write_time: true}
-  load_data: {copy_data: true, data_dir: null, print_tree: false}
-  predictions_grid:
-    x: {size: 100}
-    y: {size: 100}
-  space:
-    x:
-      extent: [-1, 1]
-      size: 10
-    y:
-      extent: [-1, 1]
-      size: 30
-  test_functions:
-    num_functions:
-      n_x: {size: 10}
-      n_y: {size: 10}
-    type: Legendre
-    weight_function: uniform
-  variational_form: 1
-log_levels: {backend: warning, model: info}
-monitor_emit_interval: 2.0
-num_epochs: 5000
-num_steps: 3
-output_dir: /Users/thomasgaskin/utopya_output/VPINN/221014-103319_Poisson2D/data/uni0
-output_path: /Users/thomasgaskin/utopya_output/VPINN/221014-103319_Poisson2D/data/uni0/data.h5
-root_model_name: VPINN
-seed: 42
-write_every: 1
-write_start: 1
diff --git a/data/Poisson_SinSin2D/data/uni0/data.h5 b/data/Poisson_SinSin2D/data/uni0/data.h5
deleted file mode 100644
index fc5d3ae..0000000
--- a/data/Poisson_SinSin2D/data/uni0/data.h5
+++ /dev/null
@@ -1,3 +0,0 @@
-version https://git-lfs.github.com/spec/v1
-oid sha256:32bc16eaa6d5422f79f7a4fec99e43167393d969722679217d1a727bd578c3ef
-size 682320
diff --git a/data/Poisson_SinSin2D/data/uni0/out.log b/data/Poisson_SinSin2D/data/uni0/out.log
deleted file mode 100644
index 2333677..0000000
--- a/data/Poisson_SinSin2D/data/uni0/out.log
+++ /dev/null
@@ -1,67 +0,0 @@
-Log of 'out' stream of WorkerTask 'uni0'
----
-
-INFO    Using 'cpu' as training device. Number of threads: 8
-INFO    Initializing the neural net ...
-INFO    Generating data ...
-INFO    Generated data.
-INFO    Saving data ...
-INFO    All data saved.
-INFO    Initialising the model 'VPINN' ...
-INFO    Now commencing training for 5000 epochs ...
-PROGRESS    Completed epoch 0 / 5000;    current loss: 7.756664752960205
-PROGRESS    Completed epoch 100 / 5000;    current loss: 3.416893720626831
-PROGRESS    Completed epoch 200 / 5000;    current loss: 3.4157185554504395
-PROGRESS    Completed epoch 300 / 5000;    current loss: 3.4136900901794434
-PROGRESS    Completed epoch 400 / 5000;    current loss: 3.4063453674316406
-PROGRESS    Completed epoch 500 / 5000;    current loss: 3.2986538410186768
-PROGRESS    Completed epoch 600 / 5000;    current loss: 2.8223323822021484
-PROGRESS    Completed epoch 700 / 5000;    current loss: 2.561511754989624
-PROGRESS    Completed epoch 800 / 5000;    current loss: 2.0602829456329346
-PROGRESS    Completed epoch 900 / 5000;    current loss: 1.1213481426239014
-PROGRESS    Completed epoch 1000 / 5000;    current loss: 0.9121904373168945
-PROGRESS    Completed epoch 1100 / 5000;    current loss: 0.8396868109703064
-PROGRESS    Completed epoch 1200 / 5000;    current loss: 0.7942134141921997
-PROGRESS    Completed epoch 1300 / 5000;    current loss: 0.7744991183280945
-PROGRESS    Completed epoch 1400 / 5000;    current loss: 0.763727605342865
-PROGRESS    Completed epoch 1500 / 5000;    current loss: 0.7552802562713623
-PROGRESS    Completed epoch 1600 / 5000;    current loss: 0.7524950504302979
-PROGRESS    Completed epoch 1700 / 5000;    current loss: 0.7382463812828064
-PROGRESS    Completed epoch 1800 / 5000;    current loss: 0.733777642250061
-PROGRESS    Completed epoch 1900 / 5000;    current loss: 0.7265952229499817
-PROGRESS    Completed epoch 2000 / 5000;    current loss: 0.7571361660957336
-PROGRESS    Completed epoch 2100 / 5000;    current loss: 0.7313780188560486
-PROGRESS    Completed epoch 2200 / 5000;    current loss: 0.7117726802825928
-PROGRESS    Completed epoch 2300 / 5000;    current loss: 0.7074429988861084
-PROGRESS    Completed epoch 2400 / 5000;    current loss: 0.7533430457115173
-PROGRESS    Completed epoch 2500 / 5000;    current loss: 0.6994838118553162
-PROGRESS    Completed epoch 2600 / 5000;    current loss: 0.6953453421592712
-PROGRESS    Completed epoch 2700 / 5000;    current loss: 0.6915191411972046
-PROGRESS    Completed epoch 2800 / 5000;    current loss: 0.6879898905754089
-PROGRESS    Completed epoch 2900 / 5000;    current loss: 0.6846378445625305
-PROGRESS    Completed epoch 3000 / 5000;    current loss: 0.684135913848877
-PROGRESS    Completed epoch 3100 / 5000;    current loss: 0.6779081225395203
-PROGRESS    Completed epoch 3200 / 5000;    current loss: 0.6860384345054626
-PROGRESS    Completed epoch 3300 / 5000;    current loss: 0.6743940114974976
-PROGRESS    Completed epoch 3400 / 5000;    current loss: 0.7377777695655823
-PROGRESS    Completed epoch 3500 / 5000;    current loss: 0.6666662693023682
-PROGRESS    Completed epoch 3600 / 5000;    current loss: 0.6634900569915771
-PROGRESS    Completed epoch 3700 / 5000;    current loss: 0.6611806750297546
-PROGRESS    Completed epoch 3800 / 5000;    current loss: 0.6601096391677856
-PROGRESS    Completed epoch 3900 / 5000;    current loss: 0.6567903757095337
-PROGRESS    Completed epoch 4000 / 5000;    current loss: 0.6549156904220581
-PROGRESS    Completed epoch 4100 / 5000;    current loss: 0.6583942770957947
-PROGRESS    Completed epoch 4200 / 5000;    current loss: 0.6511828899383545
-PROGRESS    Completed epoch 4300 / 5000;    current loss: 0.6498849987983704
-PROGRESS    Completed epoch 4400 / 5000;    current loss: 0.6479568481445312
-PROGRESS    Completed epoch 4500 / 5000;    current loss: 0.646499752998352
-PROGRESS    Completed epoch 4600 / 5000;    current loss: 0.6451282501220703
-PROGRESS    Completed epoch 4700 / 5000;    current loss: 0.6449902653694153
-PROGRESS    Completed epoch 4800 / 5000;    current loss: 0.6517288684844971
-PROGRESS    Completed epoch 4900 / 5000;    current loss: 0.6413101553916931
-INFO    Simulation run finished. Generating prediction ...
-INFO    Done. Wrapping up ...
-SUCCESS    All done.
-
----
-end of log. exit code: 0
diff --git a/model/DataGeneration.py b/model/DataGeneration.py
index 2db4ed4..665c7d9 100644
--- a/model/DataGeneration.py
+++ b/model/DataGeneration.py
@@ -2,7 +2,7 @@ import copy
 import logging
 import sys
 from os.path import dirname as up
-from typing import Union
+from typing import Sequence, Union

 import h5py as h5
 import paramspace
@@ -18,244 +18,181 @@ sys.path.append(up(up(__file__)))
 base = import_module_from_path(mod_path=up(up(__file__)), mod_str="include")


-# ----------------------------------------------------------------------------------------------------------------------
-# -- Load or generate grid and training data ---------------------------------------------------------------------------
-# ----------------------------------------------------------------------------------------------------------------------
-
-
-def get_data(
+def load_grid_tf_data(
+    data_dir: str,
     load_cfg: dict,
-    space_dict: dict,
-    test_func_dict: dict,
-    *,
-    solution: callable,
-    forcing: callable,
-    boundary_isel: Union[str, tuple] = None,
-    h5file: h5.File,
 ) -> dict:

-    """Returns the grid and test function data, either by loading it from a file or generating it.
-    If generated, data is to written to the output folder
-
-    :param load_cfg: load configuration, containing the path to the data file. If the path is None, data is
-         automatically generated. If the ``copy_data`` entry is true, data will be copied and written to the
-         output directory; however, this is false by default to save disk space.
-         The configuration also contains further kwargs, passed to the loader.
-    :param space_dict: the dictionary containing the space configuration
-    :param test_func_dict: the dictionary containing the test function configuration
-    :param solution: the explicit solution (to be evaluated on the grid boundary)
-    :param forcing: the external function
-    :param var_form: the variational form to use
-    :param boundary_isel: (optional) section of the boundary to use for training. Can either be a string ('lower',
-        'upper', 'left', 'right') or a range.
-    :param h5file: the h5file to write data to. A new group is added to this file.
+    """Loads grid and test function data from an h5 file. If a test function subselection is specified,
+    selects a subset of test functions.

-    :return: data: a dictionary containing the grid and test function data
+    :param data_dir: (str) the directory containing the h5 file
+    :param load_cfg: (dict) load settings, passed to :pyfunc:utopya.eval.datamanager.DataManager.load
+    :return: a dictionary containing the grid and test function data
     """

-    # TODO: allow selection of which data to load
-    # TODO: allow passing Sequences of boundary sections, e.g. ['lower', 'upper']
-    # TODO: Do not generate separate h5Group?
-    # TODO: re-writing loaded data not possible
+    log.info("   Loading data ...")

-    # Collect datasets in a dictionary, passed to the model
     data = {}

-    # The directory from which to load data
-    data_dir = load_cfg.pop("data_dir", None)
+    # A subset of test functions can be selected to reduce compute time
+    tf_sel_dict = {
+        key: val for key, val in load_cfg.pop("test_function_subset", {}).items()
+    }

-    if data_dir is not None:
+    dm = utopya.eval.datamanager.DataManager(data_dir=data_dir, out_dir=False)

-        # --------------------------------------------------------------------------------------------------------------
-        # --- Load data ------------------------------------------------------------------------------------------------
-        # --------------------------------------------------------------------------------------------------------------
+    dm.load(
+        "VPINN_data",
+        loader=load_cfg.pop("loader", "hdf5"),
+        glob_str=load_cfg.pop("glob_str", "*.h5"),
+        print_tree=load_cfg.pop("print_tree", False),
+        **load_cfg,
+    )

-        log.info("   Loading data ...")
+    for key in list(dm["VPINN_data"]["data"]["grid_test_function_data"].keys()):

-        # If data is loaded, determine whether to copy that data to the new output directory.
-        # This is false by default to save disk storage
-        copy_data = load_cfg.pop("copy_data", False)
+        # Get the dataset and convert to xr.DataArray
+        ds = dm["VPINN_data"]["data"]["grid_test_function_data"][key]
+        data[key] = ds.data

-        dm = utopya.eval.datamanager.DataManager(data_dir=data_dir, out_dir=False)
+        # These entries are xr.Datasets, rather than xr.DataArrays
+        if key in [
+            "grid_boundary",
+            "d1_test_function_values_boundary",
+        ]:
+            data[key] = ds.data.to_dataset()
+
+        # Manually copy attributes
+        for attr in [
+            ("grid_density", lambda x: float(x)),
+            ("grid_dimension", lambda x: int(x)),
+            ("space_dimensions", lambda x: [str(_) for _ in x]),
+            ("test_function_dims", lambda x: [str(_) for _ in x]),
+        ]:
+            if attr[0] in ds.attrs.keys():
+                data[key].attrs[attr[0]] = attr[1](ds.attrs[attr[0]])

-        dm.load(
-            "VPINN_data",
-            loader=load_cfg.pop("loader", "hdf5"),
-            glob_str=load_cfg.pop("glob_str", "*.h5"),
-            print_tree=load_cfg.pop("print_tree", False),
-            **load_cfg,
+    # Stack the test function indices into a single multi-index
+    for key in [
+        "test_function_values",
+        "d1_test_function_values",
+        "d2_test_function_values",
+        "d1_test_function_values_boundary",
+    ]:
+        data[key] = (
+            data[key]
+            .sel(tf_sel_dict)
+            .stack(tf_idx=data[key].attrs["test_function_dims"])
+            .transpose("tf_idx", ...)
         )

-        for key in list(dm["VPINN_data"]["data"]["data"].keys()):
-
-            # Get the dataset and convert to xr.DataArray
-            ds = dm["VPINN_data"]["data"]["data"][key]
-            data[key] = ds.data
-
-            # These entries should be xr.Datasets
-            if key in [
-                "training_data",
-                "grid_boundary",
-                "d1_test_function_values_boundary",
-            ]:
-                data[key] = ds.data.to_dataset()
-
-            # Manually copy attributes
-            for attr in [
-                ("grid_density", lambda x: float(x)),
-                ("grid_dimension", lambda x: int(x)),
-                ("space_dimensions", lambda x: [str(_) for _ in x]),
-                ("test_function_dims", lambda x: [str(_) for _ in x]),
-            ]:
-                if attr[0] in ds.attrs.keys():
-
-                    data[key].attrs[attr[0]] = attr[1](ds.attrs[attr[0]])
-
-        # Rename data
-        data["training_data"] = data["training_data"].rename(
-            {"training_data": "boundary_data"}
-        )
+    data["grid_boundary"] = data["grid_boundary"].rename(
+        {"grid_boundary": "boundary_data"}
+    )

-        # Stack the test function indices into a single multi-index
-        for key in [
-            "test_function_values",
-            "d1_test_function_values",
-            "d2_test_function_values",
-            "d1_test_function_values_boundary",
-            "f_integrated",
-        ]:
-            data[key] = (
-                data[key]
-                .stack(tf_idx=data[key].attrs["test_function_dims"])
-                .transpose("tf_idx", ...)
-            )
+    log.info("   Data loaded.")
+
+    return data

-        log.info("   All data loaded")

-        if not copy_data:
-            return data
-    else:
+def generate_grid_tf_data(
+    space_dict: dict,
+    test_func_dict: dict,
+) -> dict:

-        # --------------------------------------------------------------------------------------------------------------
-        # --- Generate data --------------------------------------------------------------------------------------------
-        # --------------------------------------------------------------------------------------------------------------
-
-        if len(space_dict) != len(test_func_dict["num_functions"]):
-            raise ValueError(
-                f"Space and test function dimensions do not match! "
-                f"Got {len(space_dict)} and {len(test_func_dict['num_functions'])}."
-            )
-
-        log.info("   Generating data ...")
-
-        log.debug("   Constructing the grid ... ")
-        grid: xr.DataArray = base.construct_grid(space_dict)
-        boundary: xr.Dataset = base.get_boundary(grid)
-        data["grid"] = grid
-        data["grid_boundary"] = boundary
-        training_boundary = (
-            boundary
-            if boundary_isel is None
-            else base.get_boundary_isel(boundary, boundary_isel, grid)
-        )
-        log.note("   Constructed the grid.")
-
-        # The test functions are only defined on [-1, 1], so a separate grid is used to generate
-        # test function values
-        tf_space_dict = paramspace.tools.recursive_replace(
-            copy.deepcopy(space_dict),
-            select_func=lambda d: "extent" in d,
-            replace_func=lambda d: d.update(dict(extent=[-1, 1])) or d,
-        )
+    """Generates grid and test function data.

-        tf_grid: xr.DataArray = base.construct_grid(tf_space_dict)
-        tf_boundary: xr.Dataset = base.get_boundary(tf_grid)
+    :param space_dict: the configuration for the space grid
+    :param test_func_dict: the configuration for the test function grid
+    :return: a dictionary containing the grid and test function data
+    """
+    data = {}

-        log.debug("   Evaluating test functions on grid ...")
-        test_function_indices = base.construct_grid(
-            test_func_dict["num_functions"], lower=1, dtype=int
-        )
-        test_function_values = base.tf_grid_evaluation(
-            tf_grid, test_function_indices, type=test_func_dict["type"], d=0
+    if len(space_dict) != len(test_func_dict["num_functions"]):
+        raise ValueError(
+            f"Space and test function dimensions do not match! "
+            f"Got {len(space_dict)} and {len(test_func_dict['num_functions'])}."
         )

-        log.note("   Evaluated the test functions.")
-        data["test_function_values"] = test_function_values.stack(
-            tf_idx=test_function_values.attrs["test_function_dims"]
-        ).transpose("tf_idx", ...)
+    log.info("   Generating grid and test function data ...")
+
+    log.debug("   Constructing the grid ... ")
+    grid: xr.DataArray = base.construct_grid(space_dict)
+    boundary: xr.Dataset = base.get_boundary(grid)
+    data["grid"] = grid
+    data["grid_boundary"] = boundary
+
+    # The test functions are only defined on [-1, 1], so a separate grid is used to generate
+    # test function values
+    log.debug("   Constructing the test function grid ...")
+    tf_space_dict = paramspace.tools.recursive_replace(
+        copy.deepcopy(space_dict),
+        select_func=lambda d: "extent" in d,
+        replace_func=lambda d: d.update(dict(extent=[-1, 1])) or d,
+    )

-        log.debug("   Evaluating test function derivatives on grid ... ")
-        d1_test_function_values = base.tf_grid_evaluation(
-            tf_grid, test_function_indices, type=test_func_dict["type"], d=1
-        )
+    tf_grid: xr.DataArray = base.construct_grid(tf_space_dict)
+    tf_boundary: xr.Dataset = base.get_boundary(tf_grid)

-        data["d1_test_function_values"] = d1_test_function_values.stack(
-            tf_idx=test_function_values.attrs["test_function_dims"]
-        ).transpose("tf_idx", ...)
+    # Evaluate the test functions on the grid, using fast grid evaluation
+    log.debug("   Evaluating test functions on grid ...")
+    test_function_indices = base.construct_grid(
+        test_func_dict["num_functions"], lower=1, dtype=int
+    )
+    test_function_values = base.tf_grid_evaluation(
+        tf_grid, test_function_indices, type=test_func_dict["type"], d=0
+    )
+    data["test_function_values"] = test_function_values.stack(
+        tf_idx=test_function_values.attrs["test_function_dims"]
+    ).transpose("tf_idx", ...)

-        d1_test_function_values_boundary = base.tf_simple_evaluation(
-            tf_boundary.sel(variable=tf_grid.attrs["space_dimensions"]),
-            test_function_indices,
-            type=test_func_dict["type"],
-            d=1,
-            core_dim="variable",
-        )
-        data[
-            "d1_test_function_values_boundary"
-        ] = d1_test_function_values_boundary.stack(
-            tf_idx=test_function_values.attrs["test_function_dims"]
-        ).transpose(
-            "tf_idx", ...
-        )
+    log.debug("   Evaluating test function derivatives on grid ... ")
+    d1_test_function_values = base.tf_grid_evaluation(
+        tf_grid, test_function_indices, type=test_func_dict["type"], d=1
+    )

-        log.debug("   Evaluating test function second derivatives on grid ... ")
-        d2_test_function_values = base.tf_grid_evaluation(
-            tf_grid, test_function_indices, type=test_func_dict["type"], d=2
-        )
-        data["d2_test_function_values"] = d2_test_function_values.stack(
-            tf_idx=test_function_values.attrs["test_function_dims"]
-        ).transpose("tf_idx", ...)
+    data["d1_test_function_values"] = d1_test_function_values.stack(
+        tf_idx=test_function_values.attrs["test_function_dims"]
+    ).transpose("tf_idx", ...)

-        log.debug("   Evaluating the external function on the grid ...")
-        f_evaluated: xr.DataArray = xr.apply_ufunc(
-            forcing, grid, input_core_dims=[["idx"]], vectorize=True
-        )
-        data["f_evaluated"] = f_evaluated
-
-        log.debug("   Integrating the function over the grid ...")
-        f_integrated = base.integrate_xr(f_evaluated, test_function_values)
-        data["f_integrated"] = f_integrated.stack(
-            tf_idx=f_integrated.attrs["test_function_dims"]
-        ).transpose("tf_idx", ...)
-
-        log.debug("   Evaluating the solution on the boundary ...")
-        training_data: xr.Dataset = xr.concat(
-            [
-                training_boundary,
-                xr.apply_ufunc(
-                    solution,
-                    training_boundary.sel(variable=grid.attrs["space_dimensions"]),
-                    input_core_dims=[["variable"]],
-                    vectorize=True,
-                ).assign_coords(variable=("variable", ["u"])),
-            ],
-            dim="variable",
-        )
-        data["training_data"] = training_data
+    log.debug("   Evaluating test function second derivatives on grid ... ")
+    d2_test_function_values = base.tf_grid_evaluation(
+        tf_grid, test_function_indices, type=test_func_dict["type"], d=2
+    )
+    data["d2_test_function_values"] = d2_test_function_values.stack(
+        tf_idx=test_function_values.attrs["test_function_dims"]
+    ).transpose("tf_idx", ...)
+
+    # Evaluate the test function derivatives on the grid boundary
+    d1_test_function_values_boundary = base.tf_simple_evaluation(
+        tf_boundary.sel(variable=tf_grid.attrs["space_dimensions"]),
+        test_function_indices,
+        type=test_func_dict["type"],
+        d=1,
+        core_dim="variable",
+    )
+    data["d1_test_function_values_boundary"] = d1_test_function_values_boundary.stack(
+        tf_idx=test_function_values.attrs["test_function_dims"]
+    ).transpose("tf_idx", ...)

-        log.info("   Generated data.")
-    # ------------------------------------------------------------------------------------------------------------------
-    # --- Set up chunked dataset to store the state data in ------------------------------------------------------------
-    # ------------------------------------------------------------------------------------------------------------------
+    log.info("   Generated grid and test function data.")
+
+    return data

-    log.info("   Saving data ... ")
-    data_group = h5file.create_group("data")

-    # ------------------------------------------------------------------------------------------------------------------
-    # --- Grid and grid boundary ---------------------------------------------------------------------------------------
-    # ------------------------------------------------------------------------------------------------------------------
+def save_grid_tf_data(data: dict, h5file: h5.File):

-    # Grid
+    """Saves the grid and test function data to a h5 file
+
+    :param data: the dictionary containing the data
+    :param h5file: the h5.File to save the data to
+    """
+
+    log.info("   Saving data ... ")
+    data_group = h5file.create_group("grid_test_function_data")
+
+    # Initialise grid dataset
     grid = data["grid"]
     dset_grid = data_group.create_dataset(
         "grid",
@@ -300,11 +237,7 @@ def get_data(
         :,
     ] = data["grid_boundary"].to_array()

-    # ------------------------------------------------------------------------------------------------------------------
-    # --- Test function values -----------------------------------------------------------------------------------------
-    # ------------------------------------------------------------------------------------------------------------------
-
-    # Test function values
+    # Initialise test function values datasets
     test_function_values = data["test_function_values"].unstack()
     dset_test_function_values = data_group.create_dataset(
         "test_function_values",
@@ -408,83 +341,123 @@ def get_data(
         :,
     ] = d1_test_function_values_boundary

-    # ------------------------------------------------------------------------------------------------------------------
-    # --- External forcing ---------------------------------------------------------------------------------------------
-    # ------------------------------------------------------------------------------------------------------------------
+    log.info("   All data saved.")

-    # External function evaluated on the grid
-    f_evaluated = data["f_evaluated"]
-    dset_f_evaluated = data_group.create_dataset(
-        "f_evaluated",
-        list(f_evaluated.sizes.values()),
-        maxshape=list(f_evaluated.sizes.values()),
-        chunks=True,
-        compression=3,
-    )
-    dset_f_evaluated.attrs["dim_names"] = [str(_) for _ in list(f_evaluated.sizes)]

-    # Set the attributes
-    for idx in list(f_evaluated.sizes):
-        dset_f_evaluated.attrs["coords_mode__" + str(idx)] = "values"
-        dset_f_evaluated.attrs["coords__" + str(idx)] = grid.coords[idx].data
-    dset_f_evaluated.attrs.update(f_evaluated.attrs)
+def get_grid_tf_data(
+    load_cfg: dict,
+    space_dict: dict,
+    test_func_dict: dict,
+    h5file: h5.File,
+) -> dict:

-    # Write the data
-    dset_f_evaluated[
-        :,
-    ] = f_evaluated
-
-    # Integral of the forcing against the test functions.
-    # This dataset is indexed by the test function indices
-    f_integrated = data["f_integrated"].unstack()
-    dset_f_integrated = data_group.create_dataset(
-        "f_integrated",
-        list(f_integrated.sizes.values()),
-        maxshape=list(f_integrated.sizes.values()),
-        chunks=True,
-        compression=3,
-    )
-    dset_f_integrated.attrs["dim_names"] = [str(_) for _ in list(f_integrated.sizes)]
+    """Returns the grid and test function data, either by loading it from a file or generating it.
+    If generated, data is written to the output folder

-    # Set attributes
-    for idx in list(f_integrated.sizes):
-        dset_f_integrated.attrs["coords_mode__" + str(idx)] = "values"
-        dset_f_integrated.attrs["coords__" + str(idx)] = f_integrated.coords[idx].data
-    dset_f_integrated.attrs.update(f_integrated.attrs)
+    :param load_cfg: load configuration, containing the path to the data file. If the path is None, data is
+         automatically generated. If the ``copy_data`` entry is true, data will be copied and written to the
+         output directory; however, this is false by default to save disk space.
+         The configuration also contains further kwargs, passed to the loader.
+    :param space_dict: the dictionary containing the space configuration
+    :param test_func_dict: the dictionary containing the test function configuration
+    :param h5file: the h5file to write data to. A new group is added to this file.

-    # Write data
-    dset_f_integrated[
-        :,
-    ] = f_integrated
-
-    # ------------------------------------------------------------------------------------------------------------------
-    # --- Boundary training data ---------------------------------------------------------------------------------------
-    # ------------------------------------------------------------------------------------------------------------------
-
-    # Training data: values of the test function on the boundary
-    training_data = data["training_data"]
-    dset_training_data = data_group.create_dataset(
-        "training_data",
-        list(training_data.sizes.values()),
-        maxshape=list(training_data.sizes.values()),
-        chunks=True,
-        compression=3,
-    )
-    dset_training_data.attrs["dim_names"] = [str(_) for _ in list(training_data.sizes)]
-    dset_training_data.attrs.update(training_data.attrs)
+    :return: data: a dictionary containing the grid and test function data
+    """

-    # Set attributes
-    dset_training_data.attrs["coords_mode__idx"] = "trivial"
-    dset_training_data.attrs["coords_mode__variable"] = "values"
-    dset_training_data.attrs["coords__variable"] = [
-        str(_) for _ in training_data.coords["variable"].data
-    ]
+    # The directory from which to load data
+    data_dir = load_cfg.pop("data_dir", None)

-    # Write data
-    dset_training_data[
-        :,
-    ] = training_data.to_array()
+    # Load data
+    if data_dir is not None:

-    log.info("   All data saved.")
+        # If data is loaded, determine whether to copy that data to the new output directory.
+        # This is false by default to save disk storage
+        copy_data = load_cfg.pop("copy_data", False)
+
+        data = load_grid_tf_data(data_dir, load_cfg)
+
+        # If data is not copied to the new output folder, return data
+        if not copy_data:
+            return data
+
+    # Generate data
+    else:
+
+        data = generate_grid_tf_data(space_dict, test_func_dict)
+
+    # Save the data and return
+    save_grid_tf_data(data, h5file)

     return data
+
+
+def get_training_data(
+    *,
+    func: callable,
+    grid: xr.DataArray,
+    boundary: xr.Dataset,
+    boundary_isel: Union[Sequence[Union[str, slice]], str, slice, None],
+) -> dict:
+
+    """Obtains the training data, given by the boundary conditions on a specified grid boundary.
+
+    :param func: the function to evaluate on the boundary
+    :param grid: the grid data
+    :param boundary: the grid boundary
+    :param boundary_isel: the boundary selection, i.e. which part of the boundary to use as training data
+    :return: a dictionary containing the training data
+    """
+
+    # Get the training boundary
+    log.debug("   Obtaining the training boundary ...")
+    training_boundary = (
+        boundary
+        if boundary_isel is None
+        else base.get_boundary_isel(boundary, boundary_isel, grid)
+    )
+
+    # Evaluate the function on the training boundary
+    log.debug("   Evaluating the solution on the boundary ...")
+    training_data: xr.Dataset = xr.concat(
+        [
+            training_boundary,
+            xr.apply_ufunc(
+                func,
+                training_boundary.sel(variable=grid.attrs["space_dimensions"]),
+                input_core_dims=[["variable"]],
+                vectorize=True,
+            ).assign_coords(variable=("variable", ["u"])),
+        ],
+        dim="variable",
+    )
+
+    return dict(training_data=training_data)
+
+
+def get_forcing_data(
+    *,
+    func: callable,
+    grid: xr.DataArray,
+    test_function_values: xr.DataArray,
+) -> dict:
+
+    """Integrates a function against an xr.DataArray of test functions.
+
+    :param func: the function to integrate
+    :param grid: the grid
+    :param test_function_values: the test function values
+    :return: a dictionary containing the function integrated
+    """
+
+    log.debug("   Evaluating the external function on the grid ...")
+    f_evaluated: xr.DataArray = xr.apply_ufunc(
+        func, grid, input_core_dims=[["idx"]], vectorize=True
+    )
+
+    log.debug("   Integrating the function over the grid ...")
+    f_integrated: xr.DataArray = base.integrate_xr(
+        f_evaluated, test_function_values
+    ).transpose("tf_idx", ...)
+
+    return dict(f_evaluated=f_evaluated, f_integrated=f_integrated)
diff --git a/model/VPINN_base_plots.yml b/model/VPINN_base_plots.yml
index 0fcec96..b81a58b 100644
--- a/model/VPINN_base_plots.yml
+++ b/model/VPINN_base_plots.yml
@@ -67,6 +67,19 @@
   select_and_combine:
     base_path: *base_path

+
+# Plot a function on a two-dimensional domain
+heatmap:
+  based_on:
+    - .creator.universe
+    - .plot.facet_grid.pcolormesh
+  cmap: &cmap
+    continuous: true
+    from_values:
+      0: *darkblue
+      0.5: *red
+      1: *yellow
+
 # ======================================================================================================================
 #  â•”â•â•—â•¦  â•”â•â•—â•”â•¦â•—â•”â•â•—
 #  â• â•â•â•‘  â•‘ â•‘ â•‘ â•šâ•â•—
@@ -89,6 +102,21 @@ loss:
     set_scales:
       y: log

+f_integrated_2D:
+  based_on: heatmap
+  select:
+    data: VPINN/f_integrated
+  x: n_x
+  y: n_y
+  helpers:
+    set_tick_locators:
+      x: &formatting
+        major:
+          name: MaxNLocator
+          integer: true
+      y:
+        <<: *formatting
+
 # ======================================================================================================================
 #  â•”â•â•—â•¦â•â•—â•¦â•”â•¦â•—  â•”â•â•—â•¦  â•”â•â•—â•”â•¦â•—â•”â•â•—
 #  â•‘ â•¦â• â•¦â•â•‘ â•‘â•‘  â• â•â•â•‘  â•‘ â•‘ â•‘ â•šâ•â•—
@@ -101,7 +129,7 @@ grid1d:
     - .creator.universe
     - .plot.facet_grid.scatter
   select:
-    grid: data/grid
+    grid: grid_test_function_data/grid
   transform:
     - .isel: [!dag_tag grid, {idx: 0} ]
       kwargs:
@@ -121,7 +149,7 @@ grid_boundary1d:
     - .creator.universe
     - .plot.facet_grid.scatter
   select:
-    vals: data/grid_boundary
+    vals: grid_test_function_data/grid_boundary
   transform:
     - .isel: [ !dag_prev , { variable: 0 } ]
       kwargs:
@@ -153,7 +181,7 @@ grid2d:
     - .creator.universe
     - .plot.facet_grid.scatter
   select:
-    grid: data/grid
+    grid: grid_test_function_data/grid
   transform:
     - .isel: [!dag_tag grid , {idx: 0}]
       kwargs:
@@ -176,7 +204,7 @@ grid_boundary2d:
     - .creator.universe
     - .plot.facet_grid.scatter
   select:
-    vals: data/grid_boundary
+    vals: grid_test_function_data/grid_boundary
   transform:
     - .isel: [ !dag_prev , { variable: 0 } ]
       kwargs:
@@ -200,7 +228,7 @@ grid3d:
     - .creator.universe
     - .plot.facet_grid.scatter3d
   select:
-    grid: data/grid
+    grid: grid_test_function_data/grid
   transform:
     - .isel: [!dag_prev , {idx: 0}]
       kwargs:
@@ -235,7 +263,7 @@ grid_boundary3d:
     - .creator.universe
     - .plot.facet_grid.scatter3d
   select:
-    vals: data/grid_boundary
+    vals: grid_test_function_data/grid_boundary
   transform:
     - .isel: [ !dag_prev , { variable: 0 } ]
       kwargs:
@@ -264,22 +292,92 @@ grid_boundary3d:
 #  â• â•â•â• â•¦â•â•‘â•£  â•‘â•‘â•‘â•‘   â•‘ â•‘â•‘ â•‘â•‘â•‘â•‘â•šâ•â•—
 #  â•©  â•©â•šâ•â•šâ•â•â•â•©â•â•©â•šâ•â• â•© â•©â•šâ•â•â•â•šâ•â•šâ•â•
 # ======================================================================================================================
-line:
+
+# Plots the true and predicted solution
+predictions_1D:
   based_on:
     - .creator.universe
     - .plot.facet_grid.line
+  select:
+    prediction:
+      path: VPINN/predictions
+      transform:
+        - .data: [!dag_prev ]
+    solution:
+      path: VPINN/u_exact
+      transform:
+        - .data: [!dag_prev ]
+  transform:
+    - operation: pd.Index
+      args: [ [ 'prediction', 'exact solution' ] ]
+      kwargs:
+        name: 'kind'
+    - xr.concat: [ [ !dag_tag prediction, !dag_tag solution ], !dag_prev ]
+      tag: data
+  hue: kind

-# Plot a function on a two-dimensional domain
-heatmap:
-  based_on:
-    - .creator.universe
-    - .plot.facet_grid.pcolormesh
-  cmap: &cmap
+# Plots a 2-dimensional prediction
+predictions_2D:
+  based_on: heatmap
+  select:
+    data: VPINN/predictions
+
+# Plots the test functions
+test_functions:
+  based_on: heatmap
+  select:
+    data:
+      path: grid_test_function_data/test_function_values
+  row: n_x
+  col: n_y
+
+# Plot the x-derivatives of the test functions
+d1test_functions_x:
+  based_on: test_functions
+  select:
+    data:
+      path: grid_test_function_data/d1_test_function_values
+      transform:
+        - .sel: [!dag_prev , {idx: 0}]
+          kwargs:
+            drop: True
+  cbar_kwargs:
+    label: $\partial_x \nu_{kl}$
+
+# Plot the y-derivatives of the test functions
+d1test_functions_y:
+  based_on: test_functions
+  select:
+    data:
+      path: grid_test_function_data/d1_test_function_values
+      transform:
+        - .sel: [ !dag_prev , {idx: 1} ]
+          kwargs:
+            drop: True
+  cbar_kwargs:
+    label: $\partial_y \nu_{kl}$
+
+# Plot the norm of the gradient of the test functions
+d1test_functions_norm:
+  based_on: heatmap
+  select:
+    tf_data:
+      path: grid_test_function_data/d1_test_function_values
+  transform:
+    - squared: [!dag_tag tf_data  ]
+    - .sum: [!dag_prev , 'idx']
+      tag: data
+  row: n_x
+  col: n_y
+  cmap:
     continuous: true
     from_values:
-      0: *darkblue
-      0.5: *red
+      0: *red
+      0.5: *darkblue
       1: *yellow
+  cbar_kwargs:
+    label: $\Vert \nabla \nu_{kl} \Vert_2$
+

 #u_boundary:
 #  based_on:
diff --git a/model/VPINN_cfg.yml b/model/VPINN_cfg.yml
index 5de9815..0038e65 100644
--- a/model/VPINN_cfg.yml
+++ b/model/VPINN_cfg.yml
@@ -22,7 +22,7 @@ PDE:
   function: !param
     default: Tanh
     dtype: str
-    is_any_of: [Tanh, Tanh2D, SinSin2D, Burger1+1D, DoubleGauss1D, CubedRoot, PorousMedium]
+    is_any_of: [Tanh, Tanh2D, SinSin2D, Burger1+1D, Burger_compact, DoubleGauss1D, CubedRoot, PorousMedium]
   # Scalar parameters for the equations
   PorousMedium:
     m: 2
diff --git a/model/cfgs/Burger1+1D/eval.yml b/model/cfgs/Burger1+1D/eval.yml
index 3c357cb..b538b1f 100644
--- a/model/cfgs/Burger1+1D/eval.yml
+++ b/model/cfgs/Burger1+1D/eval.yml
@@ -10,10 +10,9 @@
   lightgreen:     &lightgreen       '#AFD8BC'
   grey:           &grey             '#3D4244'

+#
 predictions:
-  based_on: heatmap
-  select:
-    data: VPINN/predictions
+  based_on: predictions_2D

 frames:
   based_on:
@@ -32,18 +31,12 @@ loss:
     set_scales:
       y: log

-grid:
-  based_on: grid2d
-
-grid_boundary:
-  based_on: grid_boundary2d
-
 boundary_conditions:
   based_on:
     - .creator.universe
     - .plot.facet_grid.scatter
   select:
-    vals: data/training_data
+    vals: VPINN/training_data
   transform:
     - .sel: [!dag_tag vals , {'variable': 'x'}]
       kwargs:
@@ -66,35 +59,3 @@ boundary_conditions:
       0: *darkblue
       0.5: *yellow
       1: *red
-
-test_functions:
-  based_on: heatmap
-  select:
-    data:
-      path: data/test_function_values
-      transform:
-        - .isel: [!dag_prev , {n_x: !range [3], n_y: !range [3]}]
-  row: n_x
-  col: n_y
-
-# Plot the first x-derivatives of the test functions (first three test functions in each direction)
-d1test_functions_x:
-  based_on: test_functions
-  select:
-    data:
-      path: data/d1_test_function_values
-      transform:
-        - .isel: [!dag_prev , {n_x: !range [3], n_y: !range [3], idx: 0}]
-  cbar_kwargs:
-    label: $\partial_x \nu_{kl}$
-
-# Plot the first y-derivatives of the test functions
-d1test_functions_y:
-  based_on: test_functions
-  select:
-    data:
-      path: data/d1_test_function_values
-      transform:
-        - .isel: [!dag_prev , {n_x: !range [3], n_y: !range [3], idx: 1}]
-  cbar_kwargs:
-    label: $\partial_y \nu_{kl}$
diff --git a/model/cfgs/Burger1+1D/run.yml b/model/cfgs/Burger1+1D/run.yml
index 5d3e601..dd04be2 100644
--- a/model/cfgs/Burger1+1D/run.yml
+++ b/model/cfgs/Burger1+1D/run.yml
@@ -11,14 +11,20 @@ parameter_space:

       # Turn this off to save disk space: the training data is about 50 MB.
       # Turning this off will cause some plots to fail.
-      copy_data: True
+      copy_data: False
+
+      # Select a subset of the test functions from the dataset.
+      test_function_subset:
+        n_x: !slice [~, 10]
+        n_y: !slice [~, 8]
+
     space:
       x:
         extent: [-2, 4]
-        size: 300
+        size: 10
       y:
         extent: [0.1, 2]
-        size: 80
+        size: 10
     PDE:
       function: Burger1+1D
       type: Burger
@@ -26,10 +32,10 @@ parameter_space:
       type: Legendre
       num_functions:
         n_x:
-          size: 15
+          size: 3
         n_y:
-          size: 8
-      weight_function: uniform
+          size: 3
+      weight_function: exponential
     NeuralNet:
       num_layers: 4
       nodes_per_layer: 20
diff --git a/model/cfgs/Burger_compact/eval.yml b/model/cfgs/Burger_compact/eval.yml
index 3c357cb..dc5180f 100644
--- a/model/cfgs/Burger_compact/eval.yml
+++ b/model/cfgs/Burger_compact/eval.yml
@@ -32,12 +32,6 @@ loss:
     set_scales:
       y: log

-grid:
-  based_on: grid2d
-
-grid_boundary:
-  based_on: grid_boundary2d
-
 boundary_conditions:
   based_on:
     - .creator.universe
@@ -45,20 +39,25 @@ boundary_conditions:
   select:
     vals: data/training_data
   transform:
-    - .sel: [!dag_tag vals , {'variable': 'x'}]
+    - .sel: [!dag_tag vals , {variable: 'x'}]
       kwargs:
         drop: True
       tag: x
-    - .sel: [!dag_tag vals , {'variable': 'u'}]
+    - .sel: [!dag_tag vals, {variable: 'y'}]
+      kwargs:
+        drop: True
+      tag: t
+    - .sel: [!dag_tag vals , {variable: 'u'}]
       kwargs:
         drop: True
       tag: u
     - xr.Dataset:
       - x: !dag_tag x
+        t: !dag_tag t
         u: !dag_tag u
       tag: data
   x: x
-  y: u
+  y: t
   hue: u
   cmap:
     continuous: true
diff --git a/model/cfgs/Burger_compact/run.yml b/model/cfgs/Burger_compact/run.yml
index b9059ed..96e694d 100644
--- a/model/cfgs/Burger_compact/run.yml
+++ b/model/cfgs/Burger_compact/run.yml
@@ -2,7 +2,7 @@
 paths:
   model_note: Burger_compact
 parameter_space:
-  num_epochs: 1
+  num_epochs: 5000
   VPINN:
 #    load_data:
 #
@@ -15,10 +15,10 @@ parameter_space:
     space:
       x:
         extent: [-2, 4]
-        size: 15
+        size: 50
       y:
         extent: [0.1, 2]
-        size: 15
+        size: 50
     PDE:
       function: Burger_compact
       type: Burger
@@ -26,9 +26,9 @@ parameter_space:
       type: Legendre
       num_functions:
         n_x:
-          size: 15
+          size: 4
         n_y:
-          size: 8
+          size: 4
       weight_function: uniform
     NeuralNet:
       num_layers: 4
diff --git a/model/cfgs/Grid_generation_2D/eval.yml b/model/cfgs/Grid_generation_2D/eval.yml
index 9026253..8f8acab 100644
--- a/model/cfgs/Grid_generation_2D/eval.yml
+++ b/model/cfgs/Grid_generation_2D/eval.yml
@@ -1,11 +1,35 @@
 grid:
   based_on: grid2d
+  s: 0.3
+  c: black

 boundary:
   based_on: grid_boundary2d
+  s: 0.3
+  c: black
+

 test_functions/test_functions:
   based_on: test_functions
+  select:
+    data:
+      transform:
+        - .isel: [ !dag_prev , { n_x: !range [ 3 ], n_y: !range [ 3 ]} ]
+
+test_functions/d1_test_functions_x:
+  based_on: d1test_functions_x
+  select:
+    data:
+      transform:
+        - .isel: [ !dag_prev , { n_x: !range [ 3 ], n_y: !range [ 3 ], idx: 0 } ]
+          kwargs:
+            drop: true

-test_functions/d1_test_functions:
-  based_on: d1_test_functions
\ No newline at end of file
+test_functions/d1_test_functions_y:
+  based_on: d1test_functions_y
+  select:
+    data:
+      transform:
+        - .isel: [ !dag_prev , { n_x: !range [ 3 ], n_y: !range [ 3 ], idx: 1 } ]
+          kwargs:
+            drop: true
diff --git a/model/cfgs/Grid_generation_2D/run.yml b/model/cfgs/Grid_generation_2D/run.yml
index fedf16f..2a6277d 100644
--- a/model/cfgs/Grid_generation_2D/run.yml
+++ b/model/cfgs/Grid_generation_2D/run.yml
@@ -1,6 +1,6 @@
 ---
 paths:
-  model_note: grid_2d
+  model_note: Poisson_grid
 parameter_space:
   generation_run: True
   log_levels:
@@ -8,15 +8,15 @@ parameter_space:
   VPINN:
     space:
       x:
-        extent: [-2, 4]
-        size: 10
+        extent: [-1, 1]
+        size: 100
       y:
-        extent: [0.1, 2]
-        size: 10
+        extent: [-1, 1]
+        size: 100
     test_functions:
       type: Legendre
       num_functions:
         n_x:
-          size: 3
+          size: 20
         n_y:
-          size: 3
\ No newline at end of file
+          size: 20
\ No newline at end of file
diff --git a/model/cfgs/Poisson2D/eval.yml b/model/cfgs/Poisson2D/eval.yml
index 29524f9..e6a67c4 100644
--- a/model/cfgs/Poisson2D/eval.yml
+++ b/model/cfgs/Poisson2D/eval.yml
@@ -36,16 +36,16 @@ loss:
     set_scales:
       y: log

-grid:
+grid_plots/grid:
   based_on: grid2d

-grid_boundary:
+grid_plots/boundary:
   based_on: grid_boundary2d

 boundary_conditions:
   based_on: grid_boundary2d
   select:
-    vals: data/training_data
+    vals: VPINN/training_data
   transform:
     - .isel: [ !dag_prev , { variable: 0 } ]
       kwargs:
@@ -72,74 +72,47 @@ boundary_conditions:
       0.5: *yellow
       1: *red

-test_functions:
+# Plot the first three test functions in each direction
+test_function_plots/test_functions:
   based_on: heatmap
   select:
     data:
-      path: data/test_function_values
+      path: grid_test_function_data/test_function_values
       transform:
         - .isel: [!dag_prev , {n_x: !range [3], n_y: !range [4]}]
   row: n_x
   col: n_y

-# Plot the first x-derivatives of the test functions
-d1test_functions_x:
-  based_on: heatmap
+# Plot x-derivatives of the first three test functions
+test_function_plots/d1test_functions_x:
+  based_on: d1test_functions_x
   select:
     data:
-      path: data/d1_test_function_values
       transform:
         - .isel: [!dag_prev , {n_x: !range [3], n_y: !range [3], idx: 0}]
-  row: n_x
-  col: n_y
-  cbar_kwargs:
-    label: $\partial_x \nu_{kl}$
+          kwargs:
+            drop: true

-# Plot the first y-derivatives of the test functions
-d1test_functions_y:
-  based_on: heatmap
+# Plot the y-derivatives of the first three test functions
+test_function_plots/d1test_functions_y:
+  based_on:
+    - d1test_functions_y
+    - test_function_plots/d1test_functions_x
   select:
     data:
-      path: data/d1_test_function_values
       transform:
-        - .isel: [!dag_prev , {n_x: !range [3], n_y: !range [3], idx: 1}]
-  row: n_x
-  col: n_y
-  cbar_kwargs:
-    label: $\partial_y \nu_{kl}$
+        - .isel: [ !dag_prev , { n_x: !range [ 3 ], n_y: !range [ 3 ], idx: 1 } ]
+          kwargs:
+            drop: true

-# Plot the norm of the gradient of the test functions
-d1test_functions_norm:
-  based_on: heatmap
+# Plot the norm of the first three test function derivatives
+test_function_plots/d1test_functions_norm:
+  based_on: d1test_functions_norm
   select:
-    data:
-      path: data/d1_test_function_values
+    tf_vals:
       transform:
-        - .isel: [!dag_prev , {n_x: !range [3], n_y: !range [3]}]
-        - squared: [ !dag_prev ,  ]
-        - .sum: [!dag_prev , 'idx']
-  row: n_x
-  col: n_y
-  cmap:
-    continuous: true
-    from_values:
-      0: *red
-      0.5: *darkblue
-      1: *yellow
-  cbar_kwargs:
-    label: $\Vert \nabla \nu_{kl} \Vert_2$
+        - .isel: [ !dag_prev , { n_x: !range [ 3 ], n_y: !range [ 3 ], idx: 0 } ]

+# Plot the values of f integrated against all the test functions
 f_integrated:
-  based_on: heatmap
-  select:
-    data: data/f_integrated
-  x: n_x
-  y: n_y
-  helpers:
-    set_tick_locators:
-      x: &formatting
-        major:
-          name: MaxNLocator
-          integer: true
-      y:
-        <<: *formatting
+  based_on: f_integrated_2D
diff --git a/model/cfgs/Poisson2D/run.yml b/model/cfgs/Poisson2D/run.yml
index 806b47e..8e69e56 100644
--- a/model/cfgs/Poisson2D/run.yml
+++ b/model/cfgs/Poisson2D/run.yml
@@ -7,20 +7,15 @@ parameter_space:
     load_data:

       # Set to '~' (None) to use the settings below to generate data
-      data_dir: data/Poisson_SinSin2D/data/uni0
+      data_dir: data/Legendre/grid_2D/data/uni0

-      # copies to the data to the new output folder. Set to 'false' to conserve disk space.
-      # This will cause any plots requiring that data to no longer function
-      copy_data: true
+      # Set to 'true' to copy the grid and test function data to the new output folder.
+      copy_data: false

-    # Settings for the grid space
-    space:
-      x:
-        extent: [ -1, 1]
-        size: 10
-      y:
-        extent: [-1, 1]
-        size: 30
+      # The dataset contains 20 test functions in each direction; we will only use 10 in each
+      test_function_subset:
+        n_x: !slice [~, 10]
+        n_y: !slice [~, 10]

     # PDE settings
     PDE:
@@ -29,12 +24,6 @@ parameter_space:

     # Test function settings
     test_functions:
-      type: Legendre
-      num_functions:
-        n_x:
-          size: 10
-        n_y:
-          size: 10
       weight_function: uniform
     variational_form: 1

@@ -52,6 +41,8 @@ parameter_space:
     # Settings for the prediction resolution: updates the 'space' dictionary
     predictions_grid:
       x:
+        extent: [-1, 1]
         size: 100
       y:
+        extent: [-1, 1]
         size: 100
diff --git a/model/function_definitions.py b/model/function_definitions.py
index e5270d7..a0b0a47 100644
--- a/model/function_definitions.py
+++ b/model/function_definitions.py
@@ -26,9 +26,13 @@ EXAMPLES = {
     },
     "CubedRoot": {"u": lambda x: np.sign(x) * np.abs(x) ** (1.0 / 3), "f": lambda x: 1},
     "Burger1+1D": {"u": lambda x: 1.0 / (1 + x[0] ** 2), "f": lambda x: 0},
+    "Burger_compact": {
+        "u": lambda x: np.exp(-6 * x[0] ** 2),
+        "f": lambda x: 0,
+    },  # lambda x: max(-x[0]**2 + 1, 0.0)
     "PorousMedium": {
         "u": lambda x, t: max(
-            t ** (-1 / 3) * (1 - 1.0 / 12 * x**2 * t ** (-2 / 3)), 0
+            t ** (-1 / 3) * (1 - 1.0 / 12 * x**2 * t ** (-2 / 3)), 0.0
         ),
         "f": lambda x: 0,
     },
diff --git a/model/run.py b/model/run.py
index 71bc8e1..60e5506 100755
--- a/model/run.py
+++ b/model/run.py
@@ -1,7 +1,6 @@
 #!/usr/bin/env python3
 import sys
 import time
-from itertools import chain
 from os.path import dirname as up
 from typing import Union

@@ -10,6 +9,7 @@ import h5py as h5
 import numpy as np
 import ruamel.yaml as yaml
 import torch
+import utopya_backend
 import xarray as xr
 from dantro import logging
 from dantro._import_tools import import_module_from_path
@@ -22,7 +22,7 @@ base = import_module_from_path(mod_path=up(up(__file__)), mod_str="include")
 this = import_module_from_path(mod_path=up(__file__), mod_str="model")

 log = logging.getLogger(__name__)
-coloredlogs.install(fmt="%(levelname)s %(message)s", level="INFO", logger=log)
+coloredlogs.install(fmt="%(levelname)s %(message)s", level="DEBUG", logger=log)


 # ----------------------------------------------------------------------------------------------------------------------
@@ -81,7 +81,7 @@ class VPINN:
         def _tf_to_tensor(test_funcs: xr.DataArray) -> Union[None, torch.Tensor]:

             """Unpacks a DataArray of test function values and returns a stacked torch.Tensor. Shape of the
-            output is given by: [test function multi-index, coordinate-multi-index, 1]"""
+            output is given by: [test function multi-index, coordinate multi-index, 1]"""

             if test_funcs is None:
                 return None
@@ -349,7 +349,6 @@ class VPINN:
 if __name__ == "__main__":

     cfg_file_path = sys.argv[1]
-
     log.note("   Preparing model run ...")
     log.note(f"   Loading config file:\n        {cfg_file_path}")
     with open(cfg_file_path) as cfg_file:
@@ -357,6 +356,7 @@ if __name__ == "__main__":
     model_name = cfg.get("root_model_name", "VPINN")
     log.note(f"   Model name:  {model_name}")
     model_cfg = cfg[model_name]
+    logging.getLogger().setLevel(utopya_backend.get_level(cfg["log_levels"]["model"]))

     # Select the training device and number of threads to use
     device = model_cfg["Training"].get("device", None)
@@ -385,6 +385,39 @@ if __name__ == "__main__":
     h5file = h5.File(cfg["output_path"], mode="w")
     h5group = h5file.create_group(model_name)

+    # Get the data: grid, test function data, and training data. This is loaded from a file,
+    # if provided, else synthetically generated
+    data: dict = this.get_grid_tf_data(
+        model_cfg.get("load_data", {}),
+        model_cfg["space"],
+        model_cfg["test_functions"],
+        h5file=h5file,
+    )
+
+    # If a data generation run was performed, return
+    if cfg.get("generation_run", False):
+        log.success("   Grid and test function data generated.")
+        h5file.close()
+        sys.exit(0)
+
+    # Get the training data
+    data.update(
+        this.get_training_data(
+            func=this.EXAMPLES[model_cfg["PDE"]["function"]]["u"],
+            grid=data["grid"],
+            boundary=data["grid_boundary"],
+            boundary_isel=model_cfg["Training"].get("boundary", None),
+        )
+    )
+    # Get the external forcing data
+    data.update(
+        this.get_forcing_data(
+            func=this.EXAMPLES[model_cfg["PDE"]["function"]]["f"],
+            grid=data["grid"],
+            test_function_values=data["test_function_values"],
+        )
+    )
+
     eq_type: str = model_cfg["PDE"]["type"]
     var_form: int = model_cfg["variational_form"]

@@ -398,7 +431,7 @@ if __name__ == "__main__":
     # Initialise the neural net
     log.info("   Initializing the neural net ...")
     net = base.NeuralNet(
-        input_size=len(model_cfg["space"]),
+        input_size=data["grid"].attrs["grid_dimension"],
         output_size=1,
         eq_type=eq_type,
         var_form=var_form,
@@ -406,20 +439,6 @@ if __name__ == "__main__":
         **model_cfg["NeuralNet"],
     )

-    test_func_dict = model_cfg["test_functions"]
-
-    # Get the data: grid, test function data, and training data. This is loaded from a file,
-    # if provided, else synthetically generated
-    data: dict = this.get_data(
-        model_cfg.get("load_data", {}),
-        model_cfg["space"],
-        test_func_dict,
-        solution=this.EXAMPLES[model_cfg["PDE"]["function"]]["u"],
-        forcing=this.EXAMPLES[model_cfg["PDE"]["function"]]["f"],
-        boundary_isel=model_cfg["Training"].get("boundary", None),
-        h5file=h5file,
-    )
-
     # Initialise the model
     log.info(f"   Initialising the model '{model_name}' ...")
     model = VPINN(
@@ -431,11 +450,12 @@ if __name__ == "__main__":
         write_every=cfg["write_every"],
         write_start=cfg["write_start"],
         weight_function=this.WEIGHT_FUNCTIONS[
-            test_func_dict["weight_function"].lower()
+            model_cfg["test_functions"]["weight_function"].lower()
         ],
         **data,
     )

+    # Train the model
     num_epochs = cfg["num_epochs"]
     log.info(f"   Now commencing training for {num_epochs} epochs ...")
     for _ in range(num_epochs):
@@ -452,14 +472,16 @@ if __name__ == "__main__":

     log.info("   Simulation run finished. Generating prediction ...")

-    # Get the plot grid, which can be finer than the training grid, if specified
+    # Generate a prediction using the trained network. The prediction is evaluated on a separate grid,
+    # which can be finer than the training grid, if specified. Predictions are generated on the CPU.
+    log.debug("   Generating plot grid ...")
     plot_grid = base.construct_grid(
-        recursive_update(model_cfg["space"], model_cfg.get("predictions_grid", {}))
+        recursive_update(model_cfg.get("space", {}), model_cfg.get("predictions_grid", {}))
     )

-    # Generate predictions on cpu
     net = net.to("cpu")

+    log.debug("   Evaluating the prediction on the plot grid ...")
     predictions = xr.apply_ufunc(
         lambda x: net.forward(torch.tensor(x).float()).detach().numpy(),
         plot_grid,
@@ -467,7 +489,8 @@ if __name__ == "__main__":
         input_core_dims=[["idx"]],
     )

-    log.debug("   Evaluating the solution on the grid ...")
+    # Evaluate the solution on the grid
+    log.debug("   Evaluating the solution on the plot grid ...")
     u_exact = xr.apply_ufunc(
         this.EXAMPLES[model_cfg["PDE"]["function"]]["u"],
         plot_grid,
@@ -476,6 +499,7 @@ if __name__ == "__main__":
         keep_attrs=True,
     )

+    # Save training and prediction data
     dset_u_exact = h5group.create_dataset(
         "u_exact",
         list(u_exact.sizes.values()),
@@ -513,6 +537,75 @@ if __name__ == "__main__":
         :,
     ] = predictions

+    # External function evaluated on the grid
+    f_evaluated = data["f_evaluated"]
+    dset_f_evaluated = h5group.create_dataset(
+        "f_evaluated",
+        list(f_evaluated.sizes.values()),
+        maxshape=list(f_evaluated.sizes.values()),
+        chunks=True,
+        compression=3,
+    )
+    dset_f_evaluated.attrs["dim_names"] = [str(_) for _ in list(f_evaluated.sizes)]
+
+    # Set the attributes
+    for idx in list(f_evaluated.sizes):
+        dset_f_evaluated.attrs["coords_mode__" + str(idx)] = "values"
+        dset_f_evaluated.attrs["coords__" + str(idx)] = data["grid"].coords[idx].data
+    dset_f_evaluated.attrs.update(f_evaluated.attrs)
+
+    # Write the data
+    dset_f_evaluated[
+        :,
+    ] = f_evaluated
+
+    # Integral of the forcing against the test functions.
+    # This dataset is indexed by the test function indices
+    f_integrated = data["f_integrated"].unstack()
+    dset_f_integrated = h5group.create_dataset(
+        "f_integrated",
+        list(f_integrated.sizes.values()),
+        maxshape=list(f_integrated.sizes.values()),
+        chunks=True,
+        compression=3,
+    )
+    dset_f_integrated.attrs["dim_names"] = [str(_) for _ in list(f_integrated.sizes)]
+
+    # Set attributes
+    for idx in list(f_integrated.sizes):
+        dset_f_integrated.attrs["coords_mode__" + str(idx)] = "values"
+        dset_f_integrated.attrs["coords__" + str(idx)] = f_integrated.coords[idx].data
+    dset_f_integrated.attrs.update(f_integrated.attrs)
+
+    # Write data
+    dset_f_integrated[
+        :,
+    ] = f_integrated
+
+    # Training data: values of the test function on the boundary
+    training_data = data["training_data"]
+    dset_training_data = h5group.create_dataset(
+        "training_data",
+        list(training_data.sizes.values()),
+        maxshape=list(training_data.sizes.values()),
+        chunks=True,
+        compression=3,
+    )
+    dset_training_data.attrs["dim_names"] = [str(_) for _ in list(training_data.sizes)]
+    dset_training_data.attrs.update(training_data.attrs)
+
+    # Set attributes
+    dset_training_data.attrs["coords_mode__idx"] = "trivial"
+    dset_training_data.attrs["coords_mode__variable"] = "values"
+    dset_training_data.attrs["coords__variable"] = [
+        str(_) for _ in training_data.coords["variable"].data
+    ]
+
+    # Write data
+    dset_training_data[
+        :,
+    ] = training_data.to_array()
+
     log.info("   Done. Wrapping up ...")
     h5file.close()
