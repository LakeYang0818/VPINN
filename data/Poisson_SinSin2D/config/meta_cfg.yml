---
backups: {backup_cfg_files: true, backup_executable: false, include_git_info: true}
cluster_mode: false
cluster_params:
  additional_run_dir_fstrs: ['job{job_id:}']
  env: null
  env_var_names:
    slurm: {cluster_name: SLURM_CLUSTER_NAME, custom_out_dir: UTOPIA_CLUSTER_MODE_OUT_DIR,
      job_account: SLURM_JOB_ACCOUNT, job_id: SLURM_JOB_ID, job_name: SLURM_JOB_NAME,
      node_list: SLURM_JOB_NODELIST, node_name: SLURMD_NODENAME, num_nodes: SLURM_JOB_NUM_NODES,
      num_procs: SLURM_CPUS_ON_NODE, timestamp: RUN_TIMESTAMP}
  manager: slurm
  node_list_parser_params: {slurm: condensed}
data_manager:
  create_groups:
  - {Cls: MultiverseGroup, path: multiverse}
  default_tree_cache_path: data/.tree_cache.d3
  load_cfg:
    cfg:
      glob_str: config/*.yml
      ignore: [config/parameter_space.yml, config/parameter_space_info.yml, config/full_parameter_space.yml,
        config/full_parameter_space_info.yml, config/git_info_project.yml, config/git_info_framework.yml]
      loader: yaml
      path_regex: config/(\w+)_cfg.yml
      required: true
      target_path: cfg/{match:}
    data:
      enable_mapping: true
      glob_str: data/uni*/data.h5
      loader: hdf5_proxy
      parallel: {enabled: true, min_files: 5, min_total_size: 104857600, processes: null}
      path_regex: data/uni(\d+)/data.h5
      required: true
      target_path: multiverse/{match:}/data
    pspace: {glob_str: config/parameter_space.yml, load_as_attr: true, loader: yaml_to_object,
      required: true, target_path: multiverse, unpack_data: true}
    uni_cfg:
      glob_str: data/uni*/config.yml
      loader: yaml
      parallel: {enabled: true, min_files: 1000, min_total_size: 1048576}
      path_regex: data/uni(\d+)/config.yml
      required: true
      target_path: multiverse/{match:}/cfg
  out_dir: eval/{timestamp:}
debug_level: 0
executable_control: {run_from_tmpdir: false}
parameter_space: !pspace
  VPINN:
    NeuralNet:
      activation_funcs: {0: tanh, 1: tanh, 2: tanh, 3: tanh, 4: None}
      nodes_per_layer: 20
      num_layers: 4
      optimizer: Adam
    PDE:
      Burger: {nu: 0}
      Helmholtz: {k: -2.5}
      PorousMedium: {m: 2}
      function: SinSin2D
      type: Poisson
    Training: {batch_size: 1, boundary_loss_weight: 1, device: cpu, learning_rate: 0.001,
      variational_loss_weight: 1, write_time: true}
    load_data: {copy_data: true, data_dir: null, print_tree: false}
    predictions_grid:
      x: {size: 100}
      y: {size: 100}
    space:
      x:
        extent: [-1, 1]
        size: 10
      y:
        extent: [-1, 1]
        size: 30
    test_functions:
      num_functions:
        n_x: {size: 10}
        n_y: {size: 10}
      type: Legendre
      weight_function: uniform
    variational_form: 1
  log_levels: {backend: warning, model: info}
  monitor_emit_interval: 2.0
  num_epochs: 5000
  num_steps: 3
  root_model_name: VPINN
  seed: 42
  write_every: 1
  write_start: 1
parameters_to_validate:
  [VPINN, NeuralNet, nodes_per_layer]: !is-positive-int 20
  [VPINN, NeuralNet, num_layers]: !is-positive-int 4
  [VPINN, NeuralNet, optimizer]: !param
    default: Adam
    is_any_of: [Adagrad, Adam, AdamW, SparseAdam, Adamax, ASGD, LBFGS, NAdam, RAdam,
      RMSprop, Rprop, SGD]
  [VPINN, PDE, Burger, nu]: !is-positive-or-zero 0
  [VPINN, PDE, function]: !param
    default: Tanh
    dtype: <U0
    is_any_of: [Tanh, Tanh2D, SinSin2D, Burger1+1D, DoubleGauss1D, CubedRoot, PorousMedium]
  [VPINN, PDE, type]: !param
    default: Burger
    dtype: <U0
    is_any_of: [Poisson, Burger]
  [VPINN, test_functions, type]: !param
    default: Legendre
    dtype: <U0
    is_any_of: [Legendre, Chebyshev, Sine]
  [VPINN, test_functions, weight_function]: !param
    default: exponential
    is_any_of: [uniform, exponential]
  [VPINN, variational_form]: !param
    default: 1
    is_any_of: [0, 1, 2]
paths: {model_note: Poisson2D, out_dir: ~/utopya_output}
perform_sweep: false
perform_validation: true
plot_manager:
  base_cfg_pools: [utopya_base, framework_base, project_base, model_base]
  cfg_exists_action: raise
  creator_init_kwargs:
    multiverse: {}
    pyplot: {}
    universe: {}
  out_dir: ''
  raise_exc: false
  save_plot_cfg: true
  shared_creator_init_kwargs:
    style:
      figure.figsize: [8.0, 5.0]
  use_dantro_base_cfg_pool: true
reporter:
  report_formats:
    progress_bar:
      info_fstr: '{total_progress:>5.1f}% '
      min_report_intv: 0.5
      num_cols: adaptive
      parser: progress_bar
      show_times: true
      times_fstr: '| {elapsed:>7s} elapsed | ~{est_left:>7s} left '
      times_fstr_final: '| finished in {elapsed:} '
      times_kwargs: {mode: from_buffer, progress_buffer_size: 90}
      write_to: stdout_noreturn
    report_file:
      min_num: 4
      min_report_intv: 10
      parser: report
      show_individual_runtimes: true
      task_label_plural: universes
      task_label_singular: universe
      write_to:
        file: {path: _report.txt}
    sweep_info:
      fstr: "Sweeping over the following parameter space:\n\n{sweep_info:}"
      parser: pspace_info
      write_to:
        file: {path: _sweep_info.txt, skip_if_empty: true}
        log: {lvl: 18, skip_if_empty: true}
run_kwargs: {stop_conditions: null, timeout: null}
worker_kwargs:
  forward_raw: true
  forward_streams: in_single_run
  popen_kwargs: {encoding: utf8}
  save_streams: true
  streams_log_lvl: null
worker_manager:
  interrupt_params: {exit: false, grace_period: 5.0, send_signal: SIGINT}
  lines_per_poll: 20
  nonzero_exit_handling: raise
  num_workers: auto
  periodic_task_callback: null
  poll_delay: 0.05
  rf_spec:
    after_abort: [progress_bar, report_file]
    after_work: [progress_bar, report_file]
    before_working: [sweep_info]
    monitor_updated: [progress_bar]
    task_finished: [progress_bar, report_file]
    task_spawned: [progress_bar]
    while_working: [progress_bar]
  save_streams_on: [monitor_updated]
