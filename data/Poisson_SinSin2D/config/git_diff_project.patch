diff --git a/include/grid.py b/include/grid.py
index f2e4841..bd86dab 100644
--- a/include/grid.py
+++ b/include/grid.py
@@ -88,7 +88,7 @@ def get_boundary(grid: xarray.DataArray) -> xarray.Dataset:

         return xarray.Dataset(
             coords=dict(idx=("idx", [0, 1]), variable=("variable", [x, "normals_x"])),
-            data_vars=dict(data=(["idx", "variable"], [[x_0, -1], [x_1, +1]])),
+            data_vars=dict(boundary_data=(["idx", "variable"], [[x_0, -1], [x_1, +1]])),
             attrs=grid.attrs,
         )

@@ -126,7 +126,7 @@ def get_boundary(grid: xarray.DataArray) -> xarray.Dataset:
                 variable=("variable", [x, y, "normals_x", "normals_y"]),
             ),
             data_vars=dict(
-                data=(
+                boundary_data=(
                     ["idx", "variable"],
                     np.stack([x_vals, y_vals, n_vals_x, n_vals_y], axis=1),
                 )
@@ -192,7 +192,7 @@ def get_boundary(grid: xarray.DataArray) -> xarray.Dataset:
                 variable=("variable", [x, y, z, "normals_x", "normals_y", "normals_z"]),
             ),
             data_vars=dict(
-                data=(
+                boundary_data=(
                     ["idx", "variable"],
                     np.stack(
                         [x_vals, y_vals, z_vals, n_vals_x, n_vals_y, n_vals_z], axis=1
diff --git a/include/integration.py b/include/integration.py
index 898d072..71d1e7f 100644
--- a/include/integration.py
+++ b/include/integration.py
@@ -55,6 +55,10 @@ def integrate_xr(
     """Integrate a function over the interior of the grid"""

     # TODO use weights
-    return test_function_values.attrs["grid_density"] * (test_function_values * f).isel(
+    res = test_function_values.attrs["grid_density"] * (test_function_values * f).isel(
         {val: slice(1, -1) for val in test_function_values.attrs["space_dimensions"]}
     ).sum(test_function_values.attrs["space_dimensions"])
+
+    res.attrs.update(test_function_values.attrs)
+
+    return res
diff --git a/include/neural_net.py b/include/neural_net.py
index 2a30457..9dc90b6 100644
--- a/include/neural_net.py
+++ b/include/neural_net.py
@@ -120,7 +120,7 @@ class NeuralNet(nn.Module):
                 f"Choose from: [1, 2, 3]"
             )

-        super(NeuralNet, self).__init__()
+        super().__init__()
         self.flatten = nn.Flatten()

         self.input_dim = input_size
diff --git a/model/DataGeneration.py b/model/DataGeneration.py
index 9492751..e726095 100644
--- a/model/DataGeneration.py
+++ b/model/DataGeneration.py
@@ -3,12 +3,13 @@ import logging
 import sys
 from os.path import dirname as up
 from typing import Union
-
 import h5py as h5
 import paramspace
 import xarray as xr
 from dantro._import_tools import import_module_from_path

+import utopya.eval.datamanager
+
 log = logging.getLogger(__name__)

 sys.path.append(up(up(__file__)))
@@ -22,13 +23,12 @@ base = import_module_from_path(mod_path=up(up(__file__)), mod_str="include")


 def get_data(
-    load_from_file: str,
+    load_cfg: dict,
     space_dict: dict,
     test_func_dict: dict,
     *,
     solution: callable,
     forcing: callable,
-    var_form: int,
     boundary_isel: Union[str, tuple] = None,
     h5file: h5.File,
 ) -> dict:
@@ -36,7 +36,10 @@ def get_data(
     """Returns the grid and test function data, either by loading it from a file or generating it.
     If generated, data is to written to the output folder

-    :param load_from_file: the path to the data file. If none, data is automatically generated
+    :param load_cfg: load configuration, containing the path to the data file. If the path is None, data is
+         automatically generated. If the ``copy_data`` entry is true, data will be copied and written to the
+         output directory; however, this is false by default to save disk space.
+         The configuration also contains further kwargs, passed to the loader.
     :param space_dict: the dictionary containing the space configuration
     :param test_func_dict: the dictionary containing the test function configuration
     :param solution: the explicit solution (to be evaluated on the grid boundary)
@@ -44,35 +47,98 @@ def get_data(
     :param var_form: the variational form to use
     :param boundary_isel: (optional) section of the boundary to use for training. Can either be a string ('lower',
         'upper', 'left', 'right') or a range.
-    :param h5file: the h5file to write data to
+    :param h5file: the h5file to write data to. A new group is added to this file.
+
     :return: data: a dictionary containing the grid and test function data
     """

+    # TODO: allow selection of which data to load
+    # TODO: allow passing Sequences of boundary sections, e.g. ['lower', 'upper']
+    # TODO: Do not generate separate h5Group?
+    # TODO: re-writing loaded data not possible
+
+    # Collect datasets in a dictionary, passed to the model
     data = {}

-    if load_from_file is not None:
+    # The directory from which to load data
+    data_dir = load_cfg.pop("data_dir", None)
+
+    if data_dir is not None:
+
+        # --------------------------------------------------------------------------------------------------------------
+        # --- Load data ------------------------------------------------------------------------------------------------
+        # --------------------------------------------------------------------------------------------------------------

         log.info("   Loading data ...")
-        data = {}

-        with h5.File(load_from_file, "r") as f:
-            data["grid"] = f["grid"]
-            data["test_func_values"] = f["test_function_values"]
+        # If data is loaded, determine whether to copy that data to the new output directory.
+        # This is false by default to save disk storage
+        copy_data = load_cfg.pop("copy_data", False)

-            if var_form >= 1:
-                data["d1test_func_values"] = f["d1_test_function_values"]
+        dm = utopya.eval.datamanager.DataManager(data_dir=data_dir, out_dir=False)

-            if var_form >= 2:
-                data["d2test_func_values"] = f["d2_test_function_values"]
-                data["d1test_func_values_bd"] = f["d1_test_function_values_boundary"]
+        dm.load(
+            "VPINN_data",
+            loader=load_cfg.pop("loader", "hdf5"),
+            glob_str=load_cfg.pop("glob_str", "*.h5"),
+            print_tree=load_cfg.pop("print_tree", False),
+            **load_cfg,
+        )
+
+        for key in list(dm["VPINN_data"]["data"]["data"].keys()):
+
+            # Get the dataset and convert to xr.DataArray
+            ds = dm["VPINN_data"]["data"]["data"][key]
+            data[key] = ds.data
+
+            # These entries should be xr.Datasets
+            if key in [
+                "training_data",
+                "grid_boundary",
+                "d1_test_function_values_boundary",
+            ]:
+                data[key] = ds.data.to_dataset()
+
+            # Manually copy attributes
+            for attr in [
+                ("grid_density", lambda x: float(x)),
+                ("grid_dimension", lambda x: int(x)),
+                ("space_dimensions", lambda x: [str(_) for _ in x]),
+                ("test_function_dims", lambda x: [str(_) for _ in x]),
+            ]:
+                if attr[0] in ds.attrs.keys():
+
+                    data[key].attrs[attr[0]] = attr[1](ds.attrs[attr[0]])
+
+        # Rename data
+        data["training_data"] = data["training_data"].rename(
+            {"training_data": "boundary_data"}
+        )
+
+        # Stack the test function indices into a single multi-index
+        for key in [
+            "test_function_values",
+            "d1_test_function_values",
+            "d2_test_function_values",
+            "d1_test_function_values_boundary",
+            "f_integrated"
+        ]:
+            data[key] = (
+                data[key]
+                .stack(tf_idx=data[key].attrs["test_function_dims"])
+                .transpose("tf_idx", ...)
+            )

         log.info("   All data loaded")

+        if not copy_data:
+            return data
     else:

         # --------------------------------------------------------------------------------------------------------------
         # --- Generate data --------------------------------------------------------------------------------------------
         # --------------------------------------------------------------------------------------------------------------
+
         if len(space_dict) != len(test_func_dict["num_functions"]):
             raise ValueError(
                 f"Space and test function dimensions do not match! "
@@ -85,7 +151,7 @@ def get_data(
         grid: xr.DataArray = base.construct_grid(space_dict)
         boundary: xr.Dataset = base.get_boundary(grid)
         data["grid"] = grid
-        data["boundary"] = boundary
+        data["grid_boundary"] = boundary
         training_boundary = (
             boundary
             if boundary_isel is None
@@ -93,8 +159,8 @@ def get_data(
         )
         log.note("   Constructed the grid.")

-        # The test functions are only defined on [-1, 1]
-        # TODO Generating two grids can be expensive!
+        # The test functions are only defined on [-1, 1], so a separate grid is used to generate
+        # test function values
         tf_space_dict = paramspace.tools.recursive_replace(
             copy.deepcopy(space_dict),
             select_func=lambda d: "extent" in d,
@@ -105,8 +171,6 @@ def get_data(
         tf_boundary: xr.Dataset = base.get_boundary(tf_grid)

         log.debug("   Evaluating test functions on grid ...")
-        # The test functions are defined on
-        # TODO the test functions only need to be caluclated on the coordinates
         test_function_indices = base.construct_grid(
             test_func_dict["num_functions"], lower=1, dtype=int
         )
@@ -115,37 +179,41 @@ def get_data(
         )

         log.note("   Evaluated the test functions.")
-        data["test_func_values"] = test_function_values.stack(
+        data["test_function_values"] = test_function_values.stack(
             tf_idx=test_function_values.attrs["test_function_dims"]
-        )
+        ).transpose("tf_idx", ...)

         log.debug("   Evaluating test function derivatives on grid ... ")
-        d1test_func_values = base.tf_grid_evaluation(
+        d1_test_function_values = base.tf_grid_evaluation(
             tf_grid, test_function_indices, type=test_func_dict["type"], d=1
         )

-        data["d1test_func_values"] = d1test_func_values.stack(
+        data["d1_test_function_values"] = d1_test_function_values.stack(
             tf_idx=test_function_values.attrs["test_function_dims"]
-        )
+        ).transpose("tf_idx", ...)

-        d1test_func_values_boundary = base.tf_simple_evaluation(
+        d1_test_function_values_boundary = base.tf_simple_evaluation(
             tf_boundary.sel(variable=tf_grid.attrs["space_dimensions"]),
             test_function_indices,
             type=test_func_dict["type"],
             d=1,
             core_dim="variable",
         )
-        data["d1test_func_values_boundary"] = d1test_func_values_boundary.stack(
+        data[
+            "d1_test_function_values_boundary"
+        ] = d1_test_function_values_boundary.stack(
             tf_idx=test_function_values.attrs["test_function_dims"]
-        ).transpose("tf_idx", ...)
+        ).transpose(
+            "tf_idx", ...
+        )

         log.debug("   Evaluating test function second derivatives on grid ... ")
-        d2test_func_values = base.tf_grid_evaluation(
+        d2_test_function_values = base.tf_grid_evaluation(
             tf_grid, test_function_indices, type=test_func_dict["type"], d=2
         )
-        data["d2test_func_values"] = d2test_func_values.stack(
+        data["d2_test_function_values"] = d2_test_function_values.stack(
             tf_idx=test_function_values.attrs["test_function_dims"]
-        )
+        ).transpose("tf_idx", ...)

         log.debug("   Evaluating the external function on the grid ...")
         f_evaluated: xr.DataArray = xr.apply_ufunc(
@@ -156,11 +224,11 @@ def get_data(
         log.debug("   Integrating the function over the grid ...")
         f_integrated = base.integrate_xr(f_evaluated, test_function_values)
         data["f_integrated"] = f_integrated.stack(
-            tf_idx=test_function_values.attrs["test_function_dims"]
-        )
+            tf_idx=f_integrated.attrs["test_function_dims"]
+        ).transpose("tf_idx", ...)

         log.debug("   Evaluating the solution on the boundary ...")
-        u_boundary: xr.Dataset = xr.concat(
+        training_data: xr.Dataset = xr.concat(
             [
                 training_boundary,
                 xr.apply_ufunc(
@@ -172,205 +240,233 @@ def get_data(
             ],
             dim="variable",
         )
-        data["training_data"] = u_boundary
-
-        # --------------------------------------------------------------------------------------------------------------
-        # --- Set up chunked dataset to store the state data in --------------------------------------------------------
-        # --------------------------------------------------------------------------------------------------------------
-
-        data_group = h5file.create_group("data")
-
-        # --------------------------------------------------------------------------------------------------------------
-        # --- Grid -----------------------------------------------------------------------------------------------------
-        # --------------------------------------------------------------------------------------------------------------
-
-        dset_grid = data_group.create_dataset(
-            "grid",
-            list(grid.sizes.values()),
-            maxshape=list(grid.sizes.values()),
-            chunks=True,
-            compression=3,
-        )
-        dset_grid.attrs["dim_names"] = list(grid.sizes)
-
-        # Set attributes
-        for idx in list(grid.sizes):
-            dset_grid.attrs["coords_mode__" + str(idx)] = "values"
-            dset_grid.attrs["coords__" + str(idx)] = grid.coords[idx].data
-
-        dset_grid[
-            :,
-        ] = grid
-
-        # Training data: values of the test function on the boundary
-        dset_boundary = data_group.create_dataset(
-            "grid_boundary",
-            [
-                len(boundary),
-                list(boundary.sizes.values())[0],
-                list(boundary.sizes.values())[1],
-            ],
-            maxshape=[
-                len(boundary),
-                list(boundary.sizes.values())[0],
-                list(boundary.sizes.values())[1],
-            ],
-            chunks=True,
-            compression=3,
-        )
-        dset_boundary.attrs["dim_names"] = ["dim_name__0", "idx", "variable"]
-
-        # Set attributes
-        dset_boundary.attrs["coords_mode__idx"] = "trivial"
-        dset_boundary.attrs["coords_mode__variable"] = "values"
-        dset_boundary.attrs["coords__variable"] = [
-            str(_) for _ in boundary.coords["variable"].data
-        ]
-
-        # Write data
-        dset_boundary[
-            :,
-        ] = boundary.to_array()
-
-        # --------------------------------------------------------------------------------------------------------------
-        # --- Test function values -------------------------------------------------------------------------------------
-        # --------------------------------------------------------------------------------------------------------------
-
-        # Store the test function values
-        dset_test_func_vals = data_group.create_dataset(
-            "test_function_values",
-            test_function_values.shape,
-            maxshape=test_function_values.shape,
-            chunks=True,
-            compression=3,
-        )
-        dset_test_func_vals.attrs["dim_names"] = list(test_function_values.sizes)
-
-        # Store the first derivatives of the test function values
-        dset_d1_test_func_vals = data_group.create_dataset(
-            "d1_test_function_values",
-            d1test_func_values.shape,
-            maxshape=d1test_func_values.shape,
-            chunks=True,
-            compression=3,
-        )
-        dset_d1_test_func_vals.attrs["dim_names"] = list(d1test_func_values.sizes)
-
-        # Store the second derivatives of the test function values
-        dset_d2_test_func_vals = data_group.create_dataset(
-            "d2_test_function_values",
-            d2test_func_values.shape,
-            maxshape=d2test_func_values.shape,
-            chunks=True,
-            compression=3,
-        )
-        dset_d2_test_func_vals.attrs["dim_names"] = list(d2test_func_values.sizes)
-
-        # Set attributes
-        for idx in list(test_function_values.sizes):
-            dset_test_func_vals.attrs["coords_mode__" + str(idx)] = "values"
-            dset_test_func_vals.attrs[
-                "coords__" + str(idx)
-            ] = test_function_values.coords[idx].data
-
-            dset_d1_test_func_vals.attrs["coords_mode__" + str(idx)] = "values"
-            dset_d1_test_func_vals.attrs[
-                "coords__" + str(idx)
-            ] = d1test_func_values.coords[idx].data
-
-            dset_d2_test_func_vals.attrs["coords_mode__" + str(idx)] = "values"
-            dset_d2_test_func_vals.attrs[
-                "coords__" + str(idx)
-            ] = d2test_func_values.coords[idx].data
-
-        # Write the data
-        dset_test_func_vals[
-            :,
-        ] = test_function_values
-        dset_d1_test_func_vals[
-            :,
-        ] = d1test_func_values
-        dset_d2_test_func_vals[
-            :,
-        ] = d2test_func_values
-
-        # --------------------------------------------------------------------------------------------------------------
-        # --- External forcing -----------------------------------------------------------------------------------------
-        # --------------------------------------------------------------------------------------------------------------
-
-        # Store the forcing evaluated on the grid
-        dset_f_evaluated = data_group.create_dataset(
-            "f_evaluated",
-            list(f_evaluated.sizes.values()),
-            maxshape=list(f_evaluated.sizes.values()),
-            chunks=True,
-            compression=3,
-        )
-        dset_f_evaluated.attrs["dim_names"] = list(f_evaluated.sizes)
-
-        # Set the attributes
-        for idx in list(f_evaluated.sizes):
-            dset_f_evaluated.attrs["coords_mode__" + str(idx)] = "values"
-            dset_f_evaluated.attrs["coords__" + str(idx)] = grid.coords[idx].data
-
-        dset_f_evaluated[
-            :,
-        ] = f_evaluated
-
-        # Store the integral of the forcing against the test functions. This dataset is indexed by the
-        # test function indices
-        dset_f_integrated = data_group.create_dataset(
-            "f_integrated",
-            list(f_integrated.sizes.values()),
-            maxshape=list(f_integrated.sizes.values()),
-            chunks=True,
-            compression=3,
-        )
-        dset_f_integrated.attrs["dim_names"] = list(f_integrated.sizes)
-
-        for idx in list(f_integrated.sizes):
-            dset_f_integrated.attrs["coords_mode__" + str(idx)] = "values"
-            dset_f_integrated.attrs["coords__" + str(idx)] = f_integrated.coords[
-                idx
-            ].data
-        dset_f_integrated[
-            :,
-        ] = f_integrated
-
-        # --------------------------------------------------------------------------------------------------------------
-        # --- Exact solution -------------------------------------------------------------------------------------------
-        # --------------------------------------------------------------------------------------------------------------
-
-        # Training data: values of the test function on the boundary
-        dset_u_exact_bd = data_group.create_dataset(
-            "u_exact_boundary",
-            [
-                len(u_boundary),
-                list(u_boundary.sizes.values())[0],
-                list(u_boundary.sizes.values())[1],
-            ],
-            maxshape=[
-                len(u_boundary),
-                list(u_boundary.sizes.values())[0],
-                list(u_boundary.sizes.values())[1],
-            ],
-            chunks=True,
-            compression=3,
-        )
-        dset_u_exact_bd.attrs["dim_names"] = ["dim_name__0", "idx", "variable"]
-
-        # Set attributes
-        dset_u_exact_bd.attrs["coords_mode__idx"] = "trivial"
-        dset_u_exact_bd.attrs["coords_mode__variable"] = "values"
-        dset_u_exact_bd.attrs["coords__variable"] = [
-            str(_) for _ in u_boundary.coords["variable"].data
-        ]
-
-        # Write data
-        dset_u_exact_bd[
-            :,
-        ] = u_boundary.to_array()
-
-        log.info("   All data generated and saved.")
+        data["training_data"] = training_data
+
+        log.info("   Generated data.")
+    # ------------------------------------------------------------------------------------------------------------------
+    # --- Set up chunked dataset to store the state data in ------------------------------------------------------------
+    # ------------------------------------------------------------------------------------------------------------------
+
+    log.info("   Saving data ... ")
+    data_group = h5file.create_group("data")
+
+    # ------------------------------------------------------------------------------------------------------------------
+    # --- Grid and grid boundary ---------------------------------------------------------------------------------------
+    # ------------------------------------------------------------------------------------------------------------------
+
+    # Grid
+    grid = data["grid"]
+    dset_grid = data_group.create_dataset(
+        "grid",
+        list(grid.sizes.values()),
+        maxshape=list(grid.sizes.values()),
+        chunks=True,
+        compression=3,
+    )
+    dset_grid.attrs["dim_names"] = [str(_) for _ in list(grid.sizes)]
+
+    # Set attributes
+    for idx in list(grid.sizes):
+        dset_grid.attrs["coords_mode__" + str(idx)] = "values"
+        dset_grid.attrs["coords__" + str(idx)] = grid.coords[idx].data
+    dset_grid.attrs.update(grid.attrs)
+
+    # Write data
+    dset_grid[
+        :,
+    ] = grid
+
+    # Grid boundary
+    dset_boundary = data_group.create_dataset(
+        "grid_boundary",
+        list(data["grid_boundary"].sizes.values()),
+        maxshape=list(data["grid_boundary"].sizes.values()),
+        chunks=True,
+        compression=3,
+    )
+    dset_boundary.attrs["dim_names"] = ["idx", "variable"]
+
+    # Set attributes
+    dset_boundary.attrs["coords_mode__idx"] = "trivial"
+    dset_boundary.attrs["coords_mode__variable"] = "values"
+    dset_boundary.attrs["coords__variable"] = [
+        str(_) for _ in data["grid_boundary"].coords["variable"].data
+    ]
+    dset_boundary.attrs.update(data["grid_boundary"].attrs)
+
+    # Write data
+    dset_boundary[
+        :,
+    ] = data["grid_boundary"].to_array()
+
+    # ------------------------------------------------------------------------------------------------------------------
+    # --- Test function values -----------------------------------------------------------------------------------------
+    # ------------------------------------------------------------------------------------------------------------------
+
+    # Test function values
+    test_function_values = data["test_function_values"].unstack()
+    dset_test_function_values = data_group.create_dataset(
+        "test_function_values",
+        test_function_values.shape,
+        maxshape=test_function_values.shape,
+        chunks=True,
+        compression=3,
+    )
+    dset_test_function_values.attrs["dim_names"] = [str(_) for _ in list(test_function_values.sizes)]
+
+    # First derivatives of the test function values
+    d1_test_function_values = data["d1_test_function_values"].unstack()
+    dset_d1_test_function_values = data_group.create_dataset(
+        "d1_test_function_values",
+        d1_test_function_values.shape,
+        maxshape=d1_test_function_values.shape,
+        chunks=True,
+        compression=3,
+    )
+    dset_d1_test_function_values.attrs["dim_names"] = [str(_) for _ in list(
+        d1_test_function_values.sizes
+    )]
+
+    # Second derivatives of the test function values
+    d2_test_function_values = data["d2_test_function_values"].unstack()
+    dset_d2_test_function_values = data_group.create_dataset(
+        "d2_test_function_values",
+        d2_test_function_values.shape,
+        maxshape=d2_test_function_values.shape,
+        chunks=True,
+        compression=3,
+    )
+    dset_d2_test_function_values.attrs["dim_names"] = [str(_) for _ in list(
+        d2_test_function_values.sizes
+    )]
+
+    # First derivatives of the test function values on the boundary
+    d1_test_function_values_boundary = data["d1_test_function_values_boundary"].unstack().to_array()
+    dset_d1_test_function_values_boundary = data_group.create_dataset(
+        "d1_test_function_values_boundary",
+        list(d1_test_function_values_boundary.shape),
+        maxshape=list(d1_test_function_values_boundary.shape),
+        chunks=True,
+        compression=3,
+    )
+    dset_d1_test_function_values_boundary.attrs["dim_names"] = [str(_) for _ in list(
+        d1_test_function_values_boundary.sizes
+    )]
+
+    # Set attributes
+    for idx in list(test_function_values.sizes):
+        dset_test_function_values.attrs["coords_mode__" + str(idx)] = "values"
+        dset_test_function_values.attrs[
+            "coords__" + str(idx)
+        ] = test_function_values.coords[idx].data
+
+        dset_d1_test_function_values.attrs["coords_mode__" + str(idx)] = "values"
+        dset_d1_test_function_values.attrs[
+            "coords__" + str(idx)
+        ] = d1_test_function_values.coords[idx].data
+
+        dset_d2_test_function_values.attrs["coords_mode__" + str(idx)] = "values"
+        dset_d2_test_function_values.attrs[
+            "coords__" + str(idx)
+        ] = d2_test_function_values.coords[idx].data
+    dset_test_function_values.attrs.update(test_function_values.attrs)
+    dset_d1_test_function_values.attrs.update(d1_test_function_values.attrs)
+    dset_d2_test_function_values.attrs.update(d2_test_function_values.attrs)
+    dset_d1_test_function_values_boundary.attrs.update(
+        d1_test_function_values_boundary.attrs
+    )
+
+    # Write the data
+    dset_test_function_values[
+        :,
+    ] = test_function_values
+    dset_d1_test_function_values[
+        :,
+    ] = d1_test_function_values
+    dset_d2_test_function_values[
+        :,
+    ] = d2_test_function_values
+    dset_d1_test_function_values_boundary[
+        :,
+    ] = d1_test_function_values_boundary
+
+    # ------------------------------------------------------------------------------------------------------------------
+    # --- External forcing ---------------------------------------------------------------------------------------------
+    # ------------------------------------------------------------------------------------------------------------------
+
+    # External function evaluated on the grid
+    f_evaluated = data["f_evaluated"]
+    dset_f_evaluated = data_group.create_dataset(
+        "f_evaluated",
+        list(f_evaluated.sizes.values()),
+        maxshape=list(f_evaluated.sizes.values()),
+        chunks=True,
+        compression=3,
+    )
+    dset_f_evaluated.attrs["dim_names"] = [str(_) for _ in list(f_evaluated.sizes)]
+
+    # Set the attributes
+    for idx in list(f_evaluated.sizes):
+        dset_f_evaluated.attrs["coords_mode__" + str(idx)] = "values"
+        dset_f_evaluated.attrs["coords__" + str(idx)] = grid.coords[idx].data
+    dset_f_evaluated.attrs.update(f_evaluated.attrs)
+
+    # Write the data
+    dset_f_evaluated[
+        :,
+    ] = f_evaluated
+
+    # Integral of the forcing against the test functions.
+    # This dataset is indexed by the test function indices
+    f_integrated = data["f_integrated"].unstack()
+    dset_f_integrated = data_group.create_dataset(
+        "f_integrated",
+        list(f_integrated.sizes.values()),
+        maxshape=list(f_integrated.sizes.values()),
+        chunks=True,
+        compression=3,
+    )
+    dset_f_integrated.attrs["dim_names"] = [str(_) for _ in list(f_integrated.sizes)]
+
+    # Set attributes
+    for idx in list(f_integrated.sizes):
+        dset_f_integrated.attrs["coords_mode__" + str(idx)] = "values"
+        dset_f_integrated.attrs["coords__" + str(idx)] = f_integrated.coords[idx].data
+    dset_f_integrated.attrs.update(f_integrated.attrs)
+
+    # Write data
+    dset_f_integrated[
+        :,
+    ] = f_integrated
+
+    # ------------------------------------------------------------------------------------------------------------------
+    # --- Boundary training data ---------------------------------------------------------------------------------------
+    # ------------------------------------------------------------------------------------------------------------------
+
+    # Training data: values of the test function on the boundary
+    training_data = data["training_data"]
+    dset_training_data = data_group.create_dataset(
+        "training_data",
+        list(training_data.sizes.values()),
+        maxshape=list(training_data.sizes.values()),
+        chunks=True,
+        compression=3,
+    )
+    dset_training_data.attrs["dim_names"] = [str(_) for _ in list(training_data.sizes)]
+    dset_training_data.attrs.update(training_data.attrs)
+
+    # Set attributes
+    dset_training_data.attrs["coords_mode__idx"] = "trivial"
+    dset_training_data.attrs["coords_mode__variable"] = "values"
+    dset_training_data.attrs["coords__variable"] = [
+        str(_) for _ in training_data.coords["variable"].data
+    ]
+
+    # Write data
+    dset_training_data[
+        :,
+    ] = training_data.to_array()
+
+    log.info("   All data saved.")

     return data
diff --git a/model/VPINN_cfg.yml b/model/VPINN_cfg.yml
index f352739..5de9815 100644
--- a/model/VPINN_cfg.yml
+++ b/model/VPINN_cfg.yml
@@ -1,7 +1,9 @@
 ---
 # Data loading .........................................................................................................
-
-load_from_dir: ~
+load_data:
+  data_dir: ~ # folder containing the .h5 file
+  copy_data: False # Whether to copy the loaded data to the new directory
+  print_tree: False # other parameters here will be passed to utopya.eval.DataManager.load

 # Domain settings ......................................................................................................
 space:
@@ -11,14 +13,17 @@ space:

 # PDE parameters .......................................................................................................
 PDE:
+  # Type of equation
   type: !param
     default: Burger
     dtype: str
     is_any_of: [Poisson, Burger]
+  # External function example
   function: !param
     default: Tanh
     dtype: str
     is_any_of: [Tanh, Tanh2D, SinSin2D, Burger1+1D, DoubleGauss1D, CubedRoot, PorousMedium]
+  # Scalar parameters for the equations
   PorousMedium:
     m: 2
   Helmholtz:
diff --git a/model/cfgs/Poisson1D/run.yml b/model/cfgs/Poisson1D/run.yml
index b224988..85e7b45 100644
--- a/model/cfgs/Poisson1D/run.yml
+++ b/model/cfgs/Poisson1D/run.yml
@@ -2,12 +2,15 @@
 paths:
   model_note: Poisson1D
 parameter_space:
-  num_epochs: 5000
+  num_epochs: 1
   VPINN:
+    load_data:
+      data_dir: /Users/thomasgaskin/utopya_output/VPINN/221014-103039_Poisson1D/data/uni0
+      copy_data: True
     space:
       x:
         extent: [-1, 1 ]
-        size: 60
+        size: 6
     PDE:
       function: Tanh
       type: Poisson
@@ -15,7 +18,7 @@ parameter_space:
       type: Legendre
       num_functions:
         n_x:
-          size: 80
+          size: 8
       weight_function: uniform
     variational_form: 0
     NeuralNet:
@@ -28,7 +31,7 @@ parameter_space:
         3: tanh
         4: linear
     Training:
-      device: mps
+      device: cpu
       learning_rate: 0.001
       variational_loss_weight: 20
     predictions_grid:
diff --git a/model/cfgs/Poisson2D/eval.yml b/model/cfgs/Poisson2D/eval.yml
index 6bed6ee..29524f9 100644
--- a/model/cfgs/Poisson2D/eval.yml
+++ b/model/cfgs/Poisson2D/eval.yml
@@ -45,7 +45,7 @@ grid_boundary:
 boundary_conditions:
   based_on: grid_boundary2d
   select:
-    vals: data/u_exact_boundary
+    vals: data/training_data
   transform:
     - .isel: [ !dag_prev , { variable: 0 } ]
       kwargs:
@@ -115,7 +115,7 @@ d1test_functions_norm:
     data:
       path: data/d1_test_function_values
       transform:
-        - .isel: [!dag_prev , {n_x: !range [3], n_y: !range [3], idx: 1}]
+        - .isel: [!dag_prev , {n_x: !range [3], n_y: !range [3]}]
         - squared: [ !dag_prev ,  ]
         - .sum: [!dag_prev , 'idx']
   row: n_x
diff --git a/model/cfgs/Poisson2D/run.yml b/model/cfgs/Poisson2D/run.yml
index fc82cb1..c114d1f 100644
--- a/model/cfgs/Poisson2D/run.yml
+++ b/model/cfgs/Poisson2D/run.yml
@@ -4,6 +4,16 @@ paths:
 parameter_space:
   num_epochs: 5000
   VPINN:
+    load_data:
+
+      # Set to '~' (None) to use the settings below to generate data
+      data_dir: ~ # /Users/thomasgaskin/utopya_output/VPINN/221014-103238_Poisson2D/data/uni0
+
+      # copies to the data to the new output folder. Set to 'false' to conserve disk space.
+      # This will cause any plots requiring that data to no longer function
+      copy_data: true
+
+    # Settings for the grid space
     space:
       x:
         extent: [ -1, 1]
@@ -11,9 +21,13 @@ parameter_space:
       y:
         extent: [-1, 1]
         size: 30
+
+    # PDE settings
     PDE:
       function: SinSin2D
       type: Poisson
+
+    # Test function settings
     test_functions:
       type: Legendre
       num_functions:
@@ -23,6 +37,8 @@ parameter_space:
           size: 10
       weight_function: uniform
     variational_form: 1
+
+    # Neural net architecture
     NeuralNet:
       num_layers: 4
       nodes_per_layer: 20
@@ -32,6 +48,8 @@ parameter_space:
         2: tanh
         3: tanh
         4: None
+
+    # Settings for the prediction resolution: updates the 'space' dictionary
     predictions_grid:
       x:
         size: 100
diff --git a/model/run.py b/model/run.py
index ead5562..3bb41ac 100755
--- a/model/run.py
+++ b/model/run.py
@@ -44,10 +44,10 @@ class VPINN:
         grid: xr.DataArray,
         training_data: xr.Dataset = None,
         f_integrated: xr.DataArray,
-        test_func_values: xr.DataArray,
-        d1test_func_values: xr.DataArray = None,
-        d2test_func_values: xr.DataArray = None,
-        d1test_func_values_boundary: xr.DataArray = None,
+        test_function_values: xr.DataArray,
+        d1_test_function_values: xr.DataArray = None,
+        d2_test_function_values: xr.DataArray = None,
+        d1_test_function_values_boundary: xr.DataArray = None,
         weight_function: callable = lambda x: 1,
         **__,
     ):
@@ -144,7 +144,7 @@ class VPINN:
                     torch.from_numpy(
                         training_data.sel(
                             variable=["normals_" + str(dim)], drop=True
-                        ).data.to_numpy()
+                        ).boundary_data.to_numpy()
                     ).float()
                 )

@@ -216,7 +216,7 @@ class VPINN:
             torch.from_numpy(
                 training_data.sel(
                     variable=grid.attrs["space_dimensions"], drop=True
-                ).data.to_numpy()
+                ).boundary_data.to_numpy()
             )
             .float()
             .to(device)
@@ -231,7 +231,7 @@ class VPINN:
         # Training data (boundary conditions)
         self.training_data: torch.Tensor = (
             torch.from_numpy(
-                training_data.sel(variable=["u"], drop=True).data.to_numpy()
+                training_data.sel(variable=["u"], drop=True).boundary_data.to_numpy()
             )
             .float()
             .to(device)
@@ -244,20 +244,20 @@ class VPINN:

         # Test function values on the grid interior, indexed by their (multi-)index and grid coordinate
         self.test_func_values: torch.Tensor = _tf_to_tensor(
-            test_func_values.transpose("tf_idx", ...)
+            test_function_values
         )

         self.d1test_func_values: Union[None, torch.Tensor] = _dtf_to_tensor(
-            d1test_func_values.transpose("tf_idx", ...)
+            d1_test_function_values
         )

         self.d2test_func_values: Union[None, xr.DataArray] = _dtf_to_tensor(
-            d2test_func_values.transpose("tf_idx", ...)
+            d2_test_function_values
         )

         self.d1test_func_values_boundary: torch.Tensor = (
             torch.from_numpy(
-                d1test_func_values_boundary.transpose("tf_idx", ...)
+                d1_test_function_values_boundary
                 .to_array()
                 .squeeze()
                 .data
@@ -271,7 +271,7 @@ class VPINN:
                 torch.stack(
                     [
                         weight_function(np.array(idx))
-                        for idx in test_func_values.coords["tf_idx"].data
+                        for idx in test_function_values.coords["tf_idx"].data
                     ]
                 ),
                 (-1, 1),
@@ -359,7 +359,7 @@ if __name__ == "__main__":

     log.note("   Preparing model run ...")
     log.note(f"   Loading config file:\n        {cfg_file_path}")
-    with open(cfg_file_path, "r") as cfg_file:
+    with open(cfg_file_path) as cfg_file:
         cfg = yaml.load(cfg_file, Loader=yaml.Loader)
     model_name = cfg.get("root_model_name", "VPINN")
     log.note(f"   Model name:  {model_name}")
@@ -418,12 +418,11 @@ if __name__ == "__main__":
     # Get the data: grid, test function data, and training data. This is loaded from a file,
     # if provided, else synthetically generated
     data: dict = this.get_data(
-        model_cfg.get("load_from_file", None),
+        model_cfg.get("load_data", {}),
         model_cfg["space"],
         test_func_dict,
         solution=this.EXAMPLES[model_cfg["PDE"]["function"]]["u"],
         forcing=this.EXAMPLES[model_cfg["PDE"]["function"]]["f"],
-        var_form=model_cfg["variational_form"],
         boundary_isel=model_cfg["Training"].get("boundary", None),
         h5file=h5file,
     )
